{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Image Classification Model Training \n",
    "\n",
    "perform transfer learning or fine-tune a pre-trained image classification model using [Google Cloud Vertex AI](https://cloud.google.com/vertex-ai), [TensorFlow](https://www.tensorflow.org/) and [TensorFlow Hub](https://www.tensorflow.org/hub) with data generated from [02_image_classification_model.ipynb](src/02_image_classification_model.ipynb).\n",
    "\n",
    "References:\n",
    "\n",
    "* [Transfer learning with TensorFlow Hub  |  TensorFlow Core](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites \n",
    "\n",
    "1. Setup Google Cloud project (See [README.md](README.md) for full details )\n",
    "2. Create Vertex AI Workbench instance (See [README.md](README.md) for full details )\n",
    "3. Run notebook [01_generate_images.ipynb](01_generate_images.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "\n",
    "Run the cell below to check if required packages are installed.\n",
    "\n",
    "If any are not, they will be installed and kernel will automatically restart and show a notificaiton.\n",
    "\n",
    "If they are already installed, nothing will happen and proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "    ('PIL', 'Pillow'),\n",
    "    ('tensorflow', 'tensorflow'),\n",
    "    ('sklearn', 'scikit-learn'),\n",
    "    ('gcsfs', 'gcsfs')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TODO - SET USER FLAG (either 'test' or 'prod') #############\n",
    "user_flag = 'test'\n",
    "\n",
    "# Basic input validation\n",
    "if user_flag not in ['test', 'prod']:\n",
    "    raise ValueError(\"Invalid input. Please enter either 'test' or 'prod'.\")\n",
    "    \n",
    "print(\"=\" * 80)\n",
    "print(f\"User flag set to: {user_flag}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us-central1\"  \n",
    "REGION = 'us-central1' \n",
    "\n",
    "BUCKET_NAME = PROJECT_ID + \"-\" + \"fruit-veg-image-model\"\n",
    "\n",
    "DIR = 'data'\n",
    "LOCAL_CSV_IMAGE_DATA_PATH = 'data/labels.csv'\n",
    "\n",
    "DATASET_CSV = f\"gs://{BUCKET_NAME}/labels.csv\"\n",
    "DATASET_VALIDATION_SPLIT = 0.2  # 20% for validation\n",
    "\n",
    "# Choose values based on user flag #################\n",
    "if user_flag == 'test':\n",
    "    # REPLICA_COUNT = '4' # TODO - add to customJob config?\n",
    "    MACHINE_TYPE = 'n1-standard-8'\n",
    "elif user_flag == 'prod':\n",
    "    ## prod\n",
    "    # REPLICA_COUNT = '7' # TODO - add to customJob config?\n",
    "    MACHINE_TYPE = 'c2-standard-60'\n",
    "\n",
    "REPO_NAME = \"fruit-veg-image-model\"\n",
    "IMAGE_NAME = \"tf_training\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "MODEL_URI = \"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/5\" \n",
    "## TODO - try head-less since adding our own on top later \n",
    "# MODEL_URI = \"https://tfhub.dev/google/imagenet/resnet_v2_50/vector/5\" \n",
    "JOB_DISPLAY_NAME = 'fruit_veg_image_classification_training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "import os\n",
    "import json \n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform, storage\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile  # For creating temporary files\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import random\n",
    "from PIL import Image  # For image loading and preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a local directories for staging files \n",
    "\n",
    "* data files from creating labels.csv\n",
    "* build files for creating custom container and running a custom job \n",
    "* model training output files and example input images for local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('build'):\n",
    "    os.makedirs('build')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 1. Get the list of CURRENTLY ENABLED services\n",
    "enabled_services=$(gcloud services list --enabled | awk '{print $1}')\n",
    "\n",
    "# 2. Services we WANT to ensure are enabled\n",
    "services_to_enable=(\"storage.googleapis.com\" \"artifactregistry.googleapis.com\" \"cloudbuild.googleapis.com\")\n",
    "\n",
    "# 3. Check each desired service against the enabled list\n",
    "for service in \"${services_to_enable[@]}\"; do\n",
    "    if ! echo \"$enabled_services\" | grep -q \"$service\"; then\n",
    "        echo \"Enabling $service...\"\n",
    "        gcloud services enable \"$service\"\n",
    "    else\n",
    "        echo \"$service is already enabled.\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv labels file and upload for use in model training\n",
    "\n",
    "Create a csv file called `labels.csv` with the schema:  `gs://filename.jpg, label` \n",
    "\n",
    "This file should contain no headers and be located in GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_list(bucket_name):\n",
    "    # get list of all files from bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs()\n",
    "    file_list = ['gs://' + bucket_name + '/' + blob.name for blob in blobs]\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_list = get_file_list(BUCKET_NAME)\n",
    "file_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataframe(file_list):\n",
    "    # filter to include on filenames with jpeg filename\n",
    "    jpeg_files = [file for file in file_list if file.endswith(('.jpg', '.jpeg'))]\n",
    "    df = pd.DataFrame(jpeg_files, columns=['filename'])\n",
    "    # Extract the label by first removing the GCS prefix, then splitting the blobname, and finally extracting the relevant part\n",
    "    df['label'] = df['filename'].apply(lambda x: '_'.join(x.split('/')[-1].split('_')[:2]))   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data = create_dataframe(file_list)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if user_flag == 'test':\n",
    "    df_data.head(1000).to_csv(LOCAL_CSV_IMAGE_DATA_PATH, index=False, header=False)\n",
    "elif user_flag == 'prod':\n",
    "    df_data.to_csv(LOCAL_CSV_IMAGE_DATA_PATH, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob('labels.csv') \n",
    "blob.upload_from_filename(LOCAL_CSV_IMAGE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Preprocessing ---\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def create_tfrecord_from_gcs(gcs_path, label, tfrecord_writer, storage_client, label_map):\n",
    "    \"\"\"Creates a TFRecord from an image in GCS and writes it to the writer.\"\"\"\n",
    "    try:\n",
    "        bucket_name, blob_name = gcs_path[5:].split('/', 1)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            blob.download_to_filename(temp_file.name)\n",
    "            with open(temp_file.name, 'rb') as image_file:\n",
    "                image_string = image_file.read()\n",
    "\n",
    "        # Convert label to integer using the label map\n",
    "        label_int = label_map[label]\n",
    "\n",
    "        feature = {\n",
    "            'image/encoded': _bytes_feature(image_string),\n",
    "            'image/class/label': _int64_feature(label_int)\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        tfrecord_writer.write(example.SerializeToString())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gcs_path}: {e}\")\n",
    "\n",
    "\n",
    "def create_and_upload_tfrecord(split_name, df, bucket, storage_client, label_map):\n",
    "    \"\"\"Creates a TFRecord file from the DataFrame and uploads it to GCS.\"\"\"\n",
    "    blob_name = f\"{split_name}.tfrecord\"\n",
    "    blob = bucket.blob(blob_name)\n",
    "    writer_lock = threading.Lock()\n",
    "    error_occurred = False  # Flag to track errors\n",
    "\n",
    "    with tempfile.NamedTemporaryFile() as temp_file:\n",
    "        writer = tf.io.TFRecordWriter(temp_file.name)\n",
    "        for row in df.itertuples():\n",
    "            try:\n",
    "                create_tfrecord_from_gcs(row.image_path, row.label, writer, storage_client, label_map)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {row.image_path}: {e}\")\n",
    "                error_occurred = True  # Set the flag if an error occurs\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        # Check if any errors occurred before uploading\n",
    "        if not error_occurred:\n",
    "            blob.upload_from_filename(temp_file.name, timeout=600) # to avoid any connection timeout issues\n",
    "            print(f\"Uploaded {blob_name} to GCS.\")\n",
    "        else:\n",
    "            print(f\"Not uploading {blob_name} due to errors during TFRecord creation.\")\n",
    "\n",
    "# --- Main Preprocessing Function ---\n",
    "def preprocess_data():\n",
    "    \"\"\"Reads CSV, splits data, creates label map, and creates/uploads TFRecords.\"\"\"\n",
    "    df = pd.read_csv(DATASET_CSV, header=None, names=['image_path', 'label'])\n",
    "    train_df, val_df = train_test_split(df, test_size=DATASET_VALIDATION_SPLIT, random_state=42)\n",
    "\n",
    "    # Create a label map (dictionary mapping labels to integer IDs)\n",
    "    unique_labels = df['label'].unique()\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "    # save label map to GCS for use in prediction later \n",
    "    blob = bucket.blob(\"label_map.json\")  # Specify GCS path for label_map.json here\n",
    "    blob.upload_from_string(json.dumps(label_map), content_type='application/json')\n",
    "\n",
    "    create_and_upload_tfrecord('train', train_df, bucket, storage_client, label_map)\n",
    "    create_and_upload_tfrecord('val', val_df, bucket, storage_client, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute pre-processing \n",
    "\n",
    "Create TFRecords files in GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build/train.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Get Environment Variables\n",
    "TFRECORD_PATH = os.environ['AIP_TFRECORD_PATH']\n",
    "VALIDATION_PATH = os.environ['AIP_VALIDATION_PATH']\n",
    "OUTPUT_PATH = os.environ['AIP_OUTPUT_PATH']\n",
    "MODEL_URI = os.environ['MODEL_URI']\n",
    "\n",
    "# sanity check TF version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Load the TFRecords\n",
    "def parse_tfrecord(example):\n",
    "    feature_description = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = tf.io.decode_jpeg(example['image/encoded'], channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])  # Resize to match model input\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    label = tf.cast(example['image/class/label'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return tf.data.TFRecordDataset(filename).map(parse_tfrecord).shuffle(1000).batch(32).prefetch(1)\n",
    "\n",
    "train_dataset = get_dataset(TFRECORD_PATH)\n",
    "val_dataset = get_dataset(VALIDATION_PATH)\n",
    "\n",
    "# Load the TensorFlow Hub model\n",
    "model = hub.KerasLayer(MODEL_URI, trainable=False)\n",
    "\n",
    "# Add your custom classification head\n",
    "num_classes = 6  # CHECK - set this to the number of classes in your dataset ###########################\n",
    "model = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "# Save the model\n",
    "model.save(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile\n",
    "\n",
    "Refs:\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/training/pre-built-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build/Dockerfile\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /root\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install tensorflow-hub==0.12.0\n",
    "\n",
    "# Copy the training script\n",
    "COPY train.py /root/train.py\n",
    "\n",
    "# Define the entry point\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Container Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create docker repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create {REPO_NAME} --repository-format=docker --location={REGION} --description=\"Docker repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optional: uncomment if you'd like to sanity check \n",
    "# ! gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure auth for docker\n",
    "\n",
    "Before you push or pull container images, configure Docker to use the `gcloud` command-line tool to authenticate requests to `Artifact Registry` for your region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --region={REGION} --tag={IMAGE_URI} --timeout=1h ./build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Job Definition \n",
    "\n",
    "\n",
    "Refs \n",
    "\n",
    "* [Create custom training jobs  |  Vertex AI  |  Google Cloud](https://cloud.google.com/vertex-ai/docs/training/create-custom-job#create_custom_job-python_vertex_ai_sdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_custom_job(display_name, \n",
    "                      bucket_name, \n",
    "                      project_id, \n",
    "                      region, \n",
    "                      machine_type\n",
    "                     ):\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": IMAGE_URI,\n",
    "                \"env\": [  \n",
    "                    {\"name\": \"AIP_TFRECORD_PATH\", \"value\": f\"gs://{bucket_name}/train.tfrecord\"},\n",
    "                    {\"name\": \"AIP_VALIDATION_PATH\", \"value\": f\"gs://{bucket_name}/val.tfrecord\"},\n",
    "                    {\"name\": \"AIP_OUTPUT_PATH\", \"value\": f\"gs://{bucket_name}/output\"},\n",
    "                    {\"name\": \"AIP_MODEL_DIR\", \"value\": f\"gs://{bucket_name}/output\"},  # <- important! the model output location\n",
    "                    {\"name\": \"TFHUB_CACHE_DIR\", \"value\": f\"gs://{bucket_name}/tfhub_cache\"}, \n",
    "                    {\"name\": \"MODEL_URI\", \"value\": MODEL_URI},\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    custom_job = aiplatform.CustomJob(\n",
    "        display_name=display_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        staging_bucket=f\"gs://{bucket_name}\"\n",
    "    )\n",
    "    \n",
    "    return custom_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_job = create_custom_job(\n",
    "    display_name=JOB_DISPLAY_NAME,  \n",
    "    bucket_name=BUCKET_NAME,\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    machine_type=MACHINE_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Submit the custom job to Vertex AI\n",
    "custom_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model for local inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions \n",
    "\n",
    "For downloading model, a sample image and finally making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_blobs_with_prefix(bucket_name, prefix, local_directory):\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Skip \"directory\" objects\n",
    "        if blob.name.endswith(\"/\"):\n",
    "            continue\n",
    "\n",
    "        # Calculate the relative path within the prefix\n",
    "        relative_path = blob.name[len(prefix):] \n",
    "\n",
    "        # Create the local directory for the relative path\n",
    "        local_file_directory = os.path.join(local_directory, os.path.dirname(relative_path))\n",
    "        os.makedirs(local_file_directory, exist_ok=True)\n",
    "\n",
    "        # Download the blob\n",
    "        local_file_path = os.path.join(local_directory, relative_path)\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        print(f\"Blob {blob.name} downloaded to {local_file_path}.\")\n",
    "\n",
    "def download_random_jpg(bucket_name, filter_substring=\".jpg\"):\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    # Get list of blobs (files) with the filter_substring\n",
    "    blobs = [blob for blob in bucket.list_blobs() if filter_substring in blob.name] \n",
    "    \n",
    "    if not blobs:\n",
    "        print(\"No files found with the filter_substring:\", filter_substring)\n",
    "        return None\n",
    "    \n",
    "    # Choose a random blob\n",
    "    random_blob = random.choice(blobs)\n",
    "\n",
    "    # Download the blob\n",
    "    local_filename = random_blob.name \n",
    "    random_blob.download_to_filename(local_filename)\n",
    "    print(f\"Downloaded {local_filename} from bucket {bucket_name}\")\n",
    "\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Preprocesses an image for model prediction.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize & set to float32\n",
    "    return img_array  # Remove extra dimension (model handles batching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_blobs_with_prefix(bucket_name=BUCKET_NAME, \n",
    "                           prefix='output/', \n",
    "                           local_directory='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Prepare an image for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## download a random image \n",
    "downloaded_file = download_random_jpg(BUCKET_NAME) \n",
    "\n",
    "## and pre-process image for prediction\n",
    "preprocessed_image = preprocess_image(downloaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## display raw output\n",
    "# preprocessed_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## display image to sanity check\n",
    "display(Image.open(downloaded_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add batch dimension\n",
    "preprocessed_image = np.expand_dims(preprocessed_image, axis=0) \n",
    "\n",
    "# Get predictions\n",
    "predictions = model(preprocessed_image)\n",
    "class_probabilities = tf.nn.softmax(predictions)  # Get probabilities\n",
    "predicted_class = tf.argmax(class_probabilities).numpy()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {class_probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a single predicted class label, using the index of the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted_class_index = tf.argmax(class_probabilities).numpy()\n",
    "# print(f\"Predicted Class Index: {predicted_class_index}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get predicted class \n",
    "\n",
    "And finally download the `label_map.json` to lookup the predicted class name when making prediction so we have a useful output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(\"label_map.json\")\n",
    "label_map_json_string = blob.download_as_string()\n",
    "label_map = json.loads(label_map_json_string)\n",
    "\n",
    "predicted_class_index = tf.argmax(class_probabilities, axis=-1).numpy()[0]  # Extract scalar value\n",
    "# Get the predicted class name (assuming you have predicted_class_index)\n",
    "predicted_class_name = [label for label, index in label_map.items() if index == predicted_class_index][0]\n",
    "\n",
    "print(\"Input File:       \", downloaded_file, \"\\n\")\n",
    "print(\"=\" * 30, \" Prediction Response\", \"=\" * 30) \n",
    "print(\"\\n\", f\"Predicted Class Name: {predicted_class_name}\", \"\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleanup downloaded image\n",
    "\n",
    "Delete the downloaded image file to keep local directory clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(downloaded_file):  # Check if the file exists\n",
    "    os.remove(downloaded_file)\n",
    "    print(f\"Deleted downloaded image file: {downloaded_file}\")\n",
    "else:\n",
    "    print(f\"Downloaded image file not found: {downloaded_file}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
