{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Image Classification Model Training \n",
    "\n",
    "Perform transfer learning or fine-tune a pre-trained image classification model using [Google Cloud Vertex AI](https://cloud.google.com/vertex-ai), [TensorFlow](https://www.tensorflow.org/) and [TensorFlow Hub](https://www.tensorflow.org/hub) with data generated from [02_image_classification_model.ipynb](src/02_image_classification_model.ipynb).\n",
    "\n",
    "References:\n",
    "\n",
    "* [Transfer learning with TensorFlow Hub  |  TensorFlow Core](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n",
    "\n",
    "by: Justin Marciszewski | justinjm@google.com | AI/ML Specialist CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites \n",
    "\n",
    "1. Setup Google Cloud project (See [README.md](README.md) for full details )\n",
    "2. Create Vertex AI Workbench instance (See [README.md](README.md) for full details )\n",
    "3. Run notebook [01_generate_images.ipynb](01_generate_images.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "\n",
    "Run the cell below to check if required packages are installed.\n",
    "\n",
    "If any are not, they will be installed and kernel will automatically restart and show a notificaiton.\n",
    "\n",
    "If they are already installed, nothing will happen and proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "    ('PIL', 'Pillow'),\n",
    "    ('tensorflow', 'tensorflow'),\n",
    "    ('sklearn', 'scikit-learn'),\n",
    "    ('gcsfs', 'gcsfs')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demos-vertex-ai'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "User flag set to: test\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "## TODO - SET USER FLAG (either 'test' or 'prod') #############\n",
    "user_flag = 'test'\n",
    "\n",
    "# Basic input validation\n",
    "if user_flag not in ['test', 'prod']:\n",
    "    raise ValueError(\"Invalid input. Please enter either 'test' or 'prod'.\")\n",
    "    \n",
    "print(\"=\" * 80)\n",
    "print(f\"User flag set to: {user_flag}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us-central1\"  \n",
    "REGION = 'us-central1' \n",
    "\n",
    "EXPERIMENT = \"02\"\n",
    "SERIES = \"fruit-and-vegetable-image-model-imagen\"\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fruit-and-vegetable-image-model-imagen\"\n",
    "\n",
    "DATASET_VALIDATION_SPLIT = 0.2  # 20% for validation\n",
    "\n",
    "## Choose values based on user flag \n",
    "if user_flag == 'test':\n",
    "    MACHINE_TYPE = 'n1-standard-8'\n",
    "elif user_flag == 'prod':\n",
    "    MACHINE_TYPE = 'c2-standard-60'\n",
    "\n",
    "REPO_NAME = \"fruit-veg-image-model-imagen-repo\"\n",
    "IMAGE_NAME = \"tf_training\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "MODEL_URI = \"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/5\" \n",
    "\n",
    "DESIRED_LABELS = [\"bellpepper_ripe\", \"apple_ripe\", \"banana_ripe\", \"bellpepper_rotten\", \"apple_rotten\", \"banana_rotten\"]\n",
    "NUM_CLASSES = len(DESIRED_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "import os\n",
    "import json \n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform, storage\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile  # For creating temporary files\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import random\n",
    "from PIL import Image  # For image loading and preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "URI = f\"gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}\" # custom job -> base_output_dir = f\"{URI}/models/{TIMESTAMP}\",\n",
    "DIR = f\"temp/{EXPERIMENT}\"\n",
    "\n",
    "LOCAL_DATA_DIR = f\"{DIR}/data\"\n",
    "LOCAL_CSV_IMAGE_DATA_PATH = f\"{LOCAL_DATA_DIR}/labels.csv\"\n",
    "\n",
    "DATASET_CSV = f\"{URI}/{TIMESTAMP}/labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRAMEWORK = 'tf'\n",
    "TASK = 'classification'\n",
    "MODEL_TYPE = 'tl'\n",
    "EXPERIMENT_NAME = f'experiment-{SERIES}-{EXPERIMENT}-{FRAMEWORK}-{TASK}-{MODEL_TYPE}'\n",
    "RUN_NAME = f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a local directories for staging files \n",
    "\n",
    "* data files from creating labels.csv\n",
    "* build files for creating custom container and running a custom job \n",
    "* model training output files and example input images for local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf $LOCAL_DATA_DIR\n",
    "! mkdir -p $LOCAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIR}/build\"):\n",
    "    os.makedirs(f\"{DIR}/build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIR}/output\"):\n",
    "    os.makedirs(f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage.googleapis.com is already enabled.\n",
      "artifactregistry.googleapis.com is already enabled.\n",
      "cloudbuild.googleapis.com is already enabled.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1. Get the list of CURRENTLY ENABLED services\n",
    "enabled_services=$(gcloud services list --enabled | awk '{print $1}')\n",
    "\n",
    "# 2. Services we WANT to ensure are enabled\n",
    "services_to_enable=(\"storage.googleapis.com\" \"artifactregistry.googleapis.com\" \"cloudbuild.googleapis.com\")\n",
    "\n",
    "# 3. Check each desired service against the enabled list\n",
    "for service in \"${services_to_enable[@]}\"; do\n",
    "    if ! echo \"$enabled_services\" | grep -q \"$service\"; then\n",
    "        echo \"Enabling $service...\"\n",
    "        gcloud services enable \"$service\"\n",
    "    else\n",
    "        echo \"$service is already enabled.\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv labels file and upload for use in model training\n",
    "\n",
    "Create a csv file called `labels.csv` with the schema:  `gs://filename.jpg, label` \n",
    "\n",
    "This file should contain no headers and be located in GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_list(bucket_name):\n",
    "    # get list of all files from bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs()\n",
    "    file_list = ['gs://' + bucket_name + '/' + blob.name for blob in blobs]\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/apple_ripe_cookie_sheet_84586_20240916132503.jpg',\n",
       " 'gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/apple_ripe_cutting_board_76388_20240916132510.jpg',\n",
       " 'gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/apple_ripe_cutting_board_76575_20240916132510.jpg',\n",
       " 'gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/apple_ripe_cutting_board_85470_20240916132503.jpg',\n",
       " 'gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/banana_rotten_cutting_board_20376_20240916132510.jpg',\n",
       " 'gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/banana_rotten_cutting_board_71186_20240916132504.jpg',\n",
       " 'gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/bellpepper_ripe_white_12127_20240916132510.jpg',\n",
       " 'gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/bellpepper_ripe_white_20544_20240916132503.jpg',\n",
       " 'gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/bellpepper_rotten_white_91521_20240916132511.jpg',\n",
       " 'gs://demos-vertex-ai-fruit-and-vegetable-image-model-imagen/data/bellpepper_rotten_white_94502_20240916132503.jpg']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = get_file_list(BUCKET_NAME)\n",
    "file_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataframe(file_list):\n",
    "    # filter to include on filenames with jpeg filename\n",
    "    jpeg_files = [file for file in file_list if file.endswith(('.jpg', '.jpeg'))]\n",
    "    df = pd.DataFrame(jpeg_files, columns=['filename'])\n",
    "    # Extract the label by first removing the GCS prefix, then splitting the blobname, and finally extracting the relevant part\n",
    "    df['label'] = df['filename'].apply(lambda x: '_'.join(x.split('/')[-1].split('_')[:2]))   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs://demos-vertex-ai-fruit-and-vegetable-image...</td>\n",
       "      <td>apple_ripe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs://demos-vertex-ai-fruit-and-vegetable-image...</td>\n",
       "      <td>apple_ripe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gs://demos-vertex-ai-fruit-and-vegetable-image...</td>\n",
       "      <td>apple_ripe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gs://demos-vertex-ai-fruit-and-vegetable-image...</td>\n",
       "      <td>apple_ripe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gs://demos-vertex-ai-fruit-and-vegetable-image...</td>\n",
       "      <td>banana_rotten</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename          label\n",
       "0  gs://demos-vertex-ai-fruit-and-vegetable-image...     apple_ripe\n",
       "1  gs://demos-vertex-ai-fruit-and-vegetable-image...     apple_ripe\n",
       "2  gs://demos-vertex-ai-fruit-and-vegetable-image...     apple_ripe\n",
       "3  gs://demos-vertex-ai-fruit-and-vegetable-image...     apple_ripe\n",
       "4  gs://demos-vertex-ai-fruit-and-vegetable-image...  banana_rotten"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = create_dataframe(file_list)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if user_flag == 'test':\n",
    "    df_data.head(1000).to_csv(LOCAL_CSV_IMAGE_DATA_PATH, index=False, header=False)\n",
    "elif user_flag == 'prod':\n",
    "    df_data.to_csv(LOCAL_CSV_IMAGE_DATA_PATH, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/labels.csv\") \n",
    "blob.upload_from_filename(LOCAL_CSV_IMAGE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Preprocessing ---\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def create_tfrecord_from_gcs(gcs_path, label, tfrecord_writer, storage_client, label_map):\n",
    "    \"\"\"Creates a TFRecord from an image in GCS and writes it to the writer.\"\"\"\n",
    "    try:\n",
    "        bucket_name, blob_name = gcs_path[5:].split('/', 1)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            blob.download_to_filename(temp_file.name)\n",
    "            with open(temp_file.name, 'rb') as image_file:\n",
    "                image_string = image_file.read()\n",
    "\n",
    "        # Convert label to integer using the label map\n",
    "        label_int = label_map[label]\n",
    "\n",
    "        feature = {\n",
    "            'image/encoded': _bytes_feature(image_string),\n",
    "            'image/class/label': _int64_feature(label_int)\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        tfrecord_writer.write(example.SerializeToString())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gcs_path}: {e}\")\n",
    "\n",
    "\n",
    "def create_and_upload_tfrecord(split_name, df, bucket, storage_client, label_map):\n",
    "    \"\"\"Creates a TFRecord file from the DataFrame and uploads it to GCS.\"\"\"\n",
    "    blob_name = f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/{split_name}.tfrecord\"\n",
    "    blob = bucket.blob(blob_name)\n",
    "    writer_lock = threading.Lock()\n",
    "    error_occurred = False  # Flag to track errors\n",
    "\n",
    "    with tempfile.NamedTemporaryFile() as temp_file:\n",
    "        writer = tf.io.TFRecordWriter(temp_file.name)\n",
    "        for row in df.itertuples():\n",
    "            try:\n",
    "                create_tfrecord_from_gcs(row.image_path, row.label, writer, storage_client, label_map)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {row.image_path}: {e}\")\n",
    "                error_occurred = True  # Set the flag if an error occurs\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        # Check if any errors occurred before uploading\n",
    "        if not error_occurred:\n",
    "            blob.upload_from_filename(temp_file.name, timeout=600) # to avoid any connection timeout issues\n",
    "            print(f\"Uploaded {blob_name} to GCS.\")\n",
    "        else:\n",
    "            print(f\"Not uploading {blob_name} due to errors during TFRecord creation.\")\n",
    "\n",
    "# --- Main Preprocessing Function ---\n",
    "def preprocess_data():\n",
    "    \"\"\"Reads CSV, splits data, creates label map, and creates/uploads TFRecords.\"\"\"\n",
    "    df = pd.read_csv(DATASET_CSV, header=None, names=['image_path', 'label'])\n",
    "    train_df, val_df = train_test_split(df, test_size=DATASET_VALIDATION_SPLIT, random_state=42)\n",
    "\n",
    "    # Create a label map (dictionary mapping labels to integer IDs)\n",
    "    unique_labels = df['label'].unique()\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "    # save label map to GCS for use in prediction later \n",
    "    blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/label_map.json\")  # Specify GCS path for label_map.json here\n",
    "    blob.upload_from_string(json.dumps(label_map), content_type='application/json')\n",
    "\n",
    "    create_and_upload_tfrecord('train', train_df, bucket, storage_client, label_map)\n",
    "    create_and_upload_tfrecord('val', val_df, bucket, storage_client, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute pre-processing \n",
    "\n",
    "Create TFRecords files in GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded fruit-and-vegetable-image-model-imagen/02/20240916133751/train.tfrecord to GCS.\n",
      "Uploaded fruit-and-vegetable-image-model-imagen/02/20240916133751/val.tfrecord to GCS.\n"
     ]
    }
   ],
   "source": [
    "preprocess_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{DIR}/build/train.py\", 'w') as f:\n",
    "    f.write(\"\"\"import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Get Environment Variables\n",
    "TFRECORD_PATH = os.environ['AIP_TFRECORD_PATH']\n",
    "VALIDATION_PATH = os.environ['AIP_VALIDATION_PATH']\n",
    "OUTPUT_PATH = os.environ['AIP_OUTPUT_PATH']\n",
    "MODEL_URI = os.environ['MODEL_URI']\n",
    "NUM_CLASSES = os.environ['NUM_CLASSES']\n",
    "\n",
    "# sanity check TF version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Load the TFRecords\n",
    "def parse_tfrecord(example):\n",
    "    feature_description = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = tf.io.decode_jpeg(example['image/encoded'], channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])  # Resize to match model input\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    label = tf.cast(example['image/class/label'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return tf.data.TFRecordDataset(filename).map(parse_tfrecord).shuffle(1000).batch(32).prefetch(1)\n",
    "\n",
    "train_dataset = get_dataset(TFRECORD_PATH)\n",
    "val_dataset = get_dataset(VALIDATION_PATH)\n",
    "\n",
    "# Load the TensorFlow Hub model\n",
    "model = hub.KerasLayer(MODEL_URI, trainable=False)\n",
    "\n",
    "# Add your custom classification head\n",
    "# CHECK - ensure this is same number of classes in your dataset ###########################\n",
    "print(f\"NUM_CLASSES: {NUM_CLASSES}\")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "# Save the model\n",
    "model.save(OUTPUT_PATH)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile\n",
    "\n",
    "Refs:\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/training/pre-built-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{DIR}/build/Dockerfile\", 'w') as f:\n",
    "    f.write(\"\"\"FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /root\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install tensorflow-hub==0.12.0\n",
    "\n",
    "# Copy the training script\n",
    "COPY train.py /root/train.py\n",
    "\n",
    "# Define the entry point\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Container Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create docker repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [fruit-veg-image-model]\n",
      "Waiting for operation [projects/demos-vertex-ai/locations/us-central1/operation\n",
      "s/c85a684b-c686-4fe0-804c-f9ca4869ffa5] to complete...done.                    \n",
      "Created repository [fruit-veg-image-model].\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create {REPO_NAME} --repository-format=docker --location={REGION} --description=\"Docker repository for fruit and vegetable image model training on Vertex AI with Imagen images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optional: uncomment if you'd like to sanity check \n",
    "# ! gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure auth for docker\n",
    "\n",
    "Before you push or pull container images, configure Docker to use the `gcloud` command-line tool to authenticate requests to `Artifact Registry` for your region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 2.0 KiB before compression.\n",
      "Uploading tarball of [./temp/02/build] to [gs://demos-vertex-ai_cloudbuild/source/1726494063.145172-90e28a189c57468992ec757de2fe093a.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/demos-vertex-ai/locations/us-central1/builds/40de1923-2bcb-4a9e-8c6d-0e16b47f6861].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/40de1923-2bcb-4a9e-8c6d-0e16b47f6861?project=746038361521 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"40de1923-2bcb-4a9e-8c6d-0e16b47f6861\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://demos-vertex-ai_cloudbuild/source/1726494063.145172-90e28a189c57468992ec757de2fe093a.tgz#1726494063516487\n",
      "Copying gs://demos-vertex-ai_cloudbuild/source/1726494063.145172-90e28a189c57468992ec757de2fe093a.tgz#1726494063516487...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  4.608kB\n",
      "Step 1/5 : FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
      "latest: Pulling from vertex-ai/training/tf-cpu.2-11\n",
      "57c139bbda7e: Pulling fs layer\n",
      "b36d8262105c: Pulling fs layer\n",
      "6289371d2bb3: Pulling fs layer\n",
      "481786a11c6e: Pulling fs layer\n",
      "275a246fad1f: Pulling fs layer\n",
      "6391690ddce2: Pulling fs layer\n",
      "bbce0e9b489f: Pulling fs layer\n",
      "a7ec0c7527b6: Pulling fs layer\n",
      "7aff549e4c97: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "e4bff5ce614c: Pulling fs layer\n",
      "2999545bd192: Pulling fs layer\n",
      "bd33b5a5ec4a: Pulling fs layer\n",
      "8fea5fa58717: Pulling fs layer\n",
      "a73bd9480671: Pulling fs layer\n",
      "139c41303dbe: Pulling fs layer\n",
      "d7df78d5fe61: Pulling fs layer\n",
      "7daa1f38b6ab: Pulling fs layer\n",
      "bce9940ec07e: Pulling fs layer\n",
      "01f79f64a163: Pulling fs layer\n",
      "5113ced62272: Pulling fs layer\n",
      "aa0eae780ea1: Pulling fs layer\n",
      "caeb9ac3c484: Pulling fs layer\n",
      "481786a11c6e: Waiting\n",
      "275a246fad1f: Waiting\n",
      "6391690ddce2: Waiting\n",
      "bbce0e9b489f: Waiting\n",
      "a7ec0c7527b6: Waiting\n",
      "7aff549e4c97: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "e4bff5ce614c: Waiting\n",
      "2999545bd192: Waiting\n",
      "bd33b5a5ec4a: Waiting\n",
      "8fea5fa58717: Waiting\n",
      "a73bd9480671: Waiting\n",
      "139c41303dbe: Waiting\n",
      "d7df78d5fe61: Waiting\n",
      "7daa1f38b6ab: Waiting\n",
      "bce9940ec07e: Waiting\n",
      "01f79f64a163: Waiting\n",
      "5113ced62272: Waiting\n",
      "aa0eae780ea1: Waiting\n",
      "caeb9ac3c484: Waiting\n",
      "b36d8262105c: Verifying Checksum\n",
      "b36d8262105c: Download complete\n",
      "6289371d2bb3: Verifying Checksum\n",
      "6289371d2bb3: Download complete\n",
      "275a246fad1f: Verifying Checksum\n",
      "275a246fad1f: Download complete\n",
      "57c139bbda7e: Verifying Checksum\n",
      "57c139bbda7e: Download complete\n",
      "bbce0e9b489f: Verifying Checksum\n",
      "bbce0e9b489f: Download complete\n",
      "481786a11c6e: Verifying Checksum\n",
      "481786a11c6e: Download complete\n",
      "a7ec0c7527b6: Verifying Checksum\n",
      "a7ec0c7527b6: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "6391690ddce2: Verifying Checksum\n",
      "6391690ddce2: Download complete\n",
      "e4bff5ce614c: Verifying Checksum\n",
      "e4bff5ce614c: Download complete\n",
      "2999545bd192: Download complete\n",
      "bd33b5a5ec4a: Verifying Checksum\n",
      "bd33b5a5ec4a: Download complete\n",
      "a73bd9480671: Verifying Checksum\n",
      "a73bd9480671: Download complete\n",
      "57c139bbda7e: Pull complete\n",
      "b36d8262105c: Pull complete\n",
      "6289371d2bb3: Pull complete\n",
      "8fea5fa58717: Verifying Checksum\n",
      "8fea5fa58717: Download complete\n",
      "d7df78d5fe61: Verifying Checksum\n",
      "d7df78d5fe61: Download complete\n",
      "7daa1f38b6ab: Download complete\n",
      "bce9940ec07e: Verifying Checksum\n",
      "bce9940ec07e: Download complete\n",
      "01f79f64a163: Verifying Checksum\n",
      "01f79f64a163: Download complete\n",
      "5113ced62272: Verifying Checksum\n",
      "5113ced62272: Download complete\n",
      "aa0eae780ea1: Verifying Checksum\n",
      "aa0eae780ea1: Download complete\n",
      "caeb9ac3c484: Verifying Checksum\n",
      "caeb9ac3c484: Download complete\n",
      "139c41303dbe: Verifying Checksum\n",
      "139c41303dbe: Download complete\n",
      "481786a11c6e: Pull complete\n",
      "275a246fad1f: Pull complete\n",
      "7aff549e4c97: Verifying Checksum\n",
      "7aff549e4c97: Download complete\n",
      "6391690ddce2: Pull complete\n",
      "bbce0e9b489f: Pull complete\n",
      "a7ec0c7527b6: Pull complete\n",
      "7aff549e4c97: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "e4bff5ce614c: Pull complete\n",
      "2999545bd192: Pull complete\n",
      "bd33b5a5ec4a: Pull complete\n",
      "8fea5fa58717: Pull complete\n",
      "a73bd9480671: Pull complete\n",
      "139c41303dbe: Pull complete\n",
      "d7df78d5fe61: Pull complete\n",
      "7daa1f38b6ab: Pull complete\n",
      "bce9940ec07e: Pull complete\n",
      "01f79f64a163: Pull complete\n",
      "5113ced62272: Pull complete\n",
      "aa0eae780ea1: Pull complete\n",
      "caeb9ac3c484: Pull complete\n",
      "Digest: sha256:07e7ea696bf78f9e51c481f74c4cee100c00870cad74b04de849aea7e66f8c51\n",
      "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
      " ---> 2c270b121e81\n",
      "Step 2/5 : WORKDIR /root\n",
      " ---> Running in 8d2732ec5a25\n",
      "Removing intermediate container 8d2732ec5a25\n",
      " ---> 5f3f28da60d9\n",
      "Step 3/5 : RUN pip install tensorflow-hub==0.12.0\n",
      " ---> Running in 31516efdc8e0\n",
      "Collecting tensorflow-hub==0.12.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub==0.12.0) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub==0.12.0) (3.20.1)\n",
      "Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.8/108.8 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.12.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 31516efdc8e0\n",
      " ---> bd29511f321a\n",
      "Step 4/5 : COPY train.py /root/train.py\n",
      " ---> 4d1a74c6e005\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 22aab215228a\n",
      "Removing intermediate container 22aab215228a\n",
      " ---> 062e8bfb885b\n",
      "Successfully built 062e8bfb885b\n",
      "Successfully tagged us-central1-docker.pkg.dev/demos-vertex-ai/fruit-veg-image-model/tf_training:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/demos-vertex-ai/fruit-veg-image-model/tf_training:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/demos-vertex-ai/fruit-veg-image-model/tf_training]\n",
      "6eda90e66a7b: Preparing\n",
      "ff5043e7bbe1: Preparing\n",
      "e42695c7b436: Preparing\n",
      "e42695c7b436: Preparing\n",
      "7e34967c8575: Preparing\n",
      "03aa2a4bdb68: Preparing\n",
      "69ff3552dab2: Preparing\n",
      "bde9e2053036: Preparing\n",
      "bde9e2053036: Preparing\n",
      "b253aec57174: Preparing\n",
      "e9a5c35692b6: Preparing\n",
      "5ca5a09f80b2: Preparing\n",
      "f27306b95858: Preparing\n",
      "e96984247094: Preparing\n",
      "bf89224ff876: Preparing\n",
      "ca7739d6661c: Preparing\n",
      "ca7739d6661c: Preparing\n",
      "6afff9338181: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "380cd88b9fb2: Preparing\n",
      "25c9ddea4aaa: Preparing\n",
      "eec152ec24b8: Preparing\n",
      "dd7d6ac03700: Preparing\n",
      "be9dc4e2456b: Preparing\n",
      "ceab7f116eb5: Preparing\n",
      "bd5ff18df433: Preparing\n",
      "a27f4aa3db94: Preparing\n",
      "1a102d1cac2b: Preparing\n",
      "69ff3552dab2: Waiting\n",
      "bde9e2053036: Waiting\n",
      "b253aec57174: Waiting\n",
      "e9a5c35692b6: Waiting\n",
      "5ca5a09f80b2: Waiting\n",
      "f27306b95858: Waiting\n",
      "e96984247094: Waiting\n",
      "bf89224ff876: Waiting\n",
      "ca7739d6661c: Waiting\n",
      "6afff9338181: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "380cd88b9fb2: Waiting\n",
      "25c9ddea4aaa: Waiting\n",
      "eec152ec24b8: Waiting\n",
      "dd7d6ac03700: Waiting\n",
      "be9dc4e2456b: Waiting\n",
      "ceab7f116eb5: Waiting\n",
      "bd5ff18df433: Waiting\n",
      "a27f4aa3db94: Waiting\n",
      "1a102d1cac2b: Waiting\n",
      "6eda90e66a7b: Pushed\n",
      "ff5043e7bbe1: Pushed\n",
      "7e34967c8575: Pushed\n",
      "03aa2a4bdb68: Pushed\n",
      "e42695c7b436: Pushed\n",
      "69ff3552dab2: Pushed\n",
      "bde9e2053036: Pushed\n",
      "b253aec57174: Pushed\n",
      "e9a5c35692b6: Pushed\n",
      "f27306b95858: Pushed\n",
      "bf89224ff876: Pushed\n",
      "ca7739d6661c: Pushed\n",
      "5f70bf18a086: Layer already exists\n",
      "6afff9338181: Pushed\n",
      "eec152ec24b8: Pushed\n",
      "25c9ddea4aaa: Pushed\n",
      "be9dc4e2456b: Pushed\n",
      "ceab7f116eb5: Pushed\n",
      "bd5ff18df433: Pushed\n",
      "a27f4aa3db94: Pushed\n",
      "1a102d1cac2b: Pushed\n",
      "e96984247094: Pushed\n",
      "dd7d6ac03700: Pushed\n",
      "5ca5a09f80b2: Pushed\n",
      "380cd88b9fb2: Pushed\n",
      "latest: digest: sha256:29918d823dba0c95e74e479b7053c21f9b1b8bf50656f372798bb44d002e5284 size: 6169\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                         IMAGES                                                                                  STATUS\n",
      "40de1923-2bcb-4a9e-8c6d-0e16b47f6861  2024-09-16T13:41:03+00:00  5M51S     gs://demos-vertex-ai_cloudbuild/source/1726494063.145172-90e28a189c57468992ec757de2fe093a.tgz  us-central1-docker.pkg.dev/demos-vertex-ai/fruit-veg-image-model/tf_training (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --region={REGION} --tag={IMAGE_URI} --timeout=1h ./{DIR}/build "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Job Definition \n",
    "\n",
    "\n",
    "Refs \n",
    "\n",
    "* [Create custom training jobs  |  Vertex AI  |  Google Cloud](https://cloud.google.com/vertex-ai/docs/training/create-custom-job#create_custom_job-python_vertex_ai_sdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_custom_job(project_id, \n",
    "                      region, \n",
    "                      display_name, \n",
    "                      bucket_name,\n",
    "                      base_output_dir,\n",
    "                      machine_type,\n",
    "                      labels\n",
    "                     ):\n",
    "    \n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": IMAGE_URI,\n",
    "                \"env\": [  \n",
    "                    {\"name\": \"AIP_TFRECORD_PATH\", \"value\": f\"{URI}/{TIMESTAMP}/train.tfrecord\"},\n",
    "                    {\"name\": \"AIP_VALIDATION_PATH\", \"value\": f\"{URI}/{TIMESTAMP}/val.tfrecord\"},\n",
    "                    {\"name\": \"AIP_OUTPUT_PATH\", \"value\": f\"{base_output_dir}/output\"},\n",
    "                    {\"name\": \"AIP_MODEL_DIR\", \"value\": f\"{base_output_dir}/output\"},  # <- important! the model output location\n",
    "                    {\"name\": \"TFHUB_CACHE_DIR\", \"value\": f\"{base_output_dir}/tfhub_cache\"}, \n",
    "                    {\"name\": \"MODEL_URI\", \"value\": MODEL_URI},\n",
    "                    {\"name\": \"NUM_CLASSES\", \"value\": str(NUM_CLASSES)},\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    custom_job = aiplatform.CustomJob(\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        display_name=display_name,\n",
    "        staging_bucket=bucket_name,\n",
    "        base_output_dir=base_output_dir,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    return custom_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4 # change after dev complete\n",
    "custom_job = create_custom_job(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    display_name = f'{SERIES}_{EXPERIMENT}_{TIMESTAMP}',\n",
    "    bucket_name = BUCKET_NAME,  # aka - staging_bucket \n",
    "    base_output_dir = f\"{URI}/models/{TIMESTAMP}\", \n",
    "    machine_type=MACHINE_TYPE,\n",
    "    labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}', 'run_name' : f'{RUN_NAME}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/746038361521/locations/us-central1/customJobs/2221556749930856448\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/746038361521/locations/us-central1/customJobs/2221556749930856448')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2221556749930856448?project=746038361521\n",
      "CustomJob projects/746038361521/locations/us-central1/customJobs/2221556749930856448 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "# Submit the custom job to Vertex AI\n",
    "custom_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model for local inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions \n",
    "\n",
    "For downloading model, a sample image and finally making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_blobs_with_prefix(bucket_name, prefix, local_directory):\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Skip \"directory\" objects\n",
    "        if blob.name.endswith(\"/\"):\n",
    "            continue\n",
    "\n",
    "        # Calculate the relative path within the prefix\n",
    "        relative_path = blob.name[len(prefix):] \n",
    "\n",
    "        # Create the local directory for the relative path\n",
    "        local_file_directory = os.path.join(local_directory, os.path.dirname(relative_path))\n",
    "        os.makedirs(local_file_directory, exist_ok=True)\n",
    "\n",
    "        # Download the blob\n",
    "        local_file_path = os.path.join(local_directory, relative_path)\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        print(f\"Blob {blob.name} downloaded to {local_file_path}.\")\n",
    "\n",
    "def download_random_jpg(bucket_name, filter_substring=\".jpg\"):\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    # Get list of blobs (files) with the filter_substring\n",
    "    blobs = [blob for blob in bucket.list_blobs() if filter_substring in blob.name] \n",
    "    \n",
    "    if not blobs:\n",
    "        print(\"No files found with the filter_substring:\", filter_substring)\n",
    "        return None\n",
    "    \n",
    "    # Choose a random blob\n",
    "    random_blob = random.choice(blobs)\n",
    "\n",
    "    # Download the blob\n",
    "    local_filename = random_blob.name \n",
    "    random_blob.download_to_filename(local_filename)\n",
    "    print(f\"Downloaded {local_filename} from bucket {bucket_name}\")\n",
    "\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Preprocesses an image for model prediction.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize & set to float32\n",
    "    return img_array  # Remove extra dimension (model handles batching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_blobs_with_prefix(bucket_name=BUCKET_NAME, \n",
    "                           prefix=f\"{SERIES}/{EXPERIMENT}/models/{TIMESTAMP}/output/\", \n",
    "                           local_directory=f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Prepare an image for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## download a random image \n",
    "downloaded_file = download_random_jpg(BUCKET_NAME) \n",
    "# downloaded_file = download_random_jpg(\n",
    "#     bucket_name=BUCKET_NAME, \n",
    "#     pattern=f'({\"|\".join(DESIRED_LABELS)})(?!\\.png$)') \n",
    "\n",
    "## and pre-process image for prediction\n",
    "preprocessed_image = preprocess_image(downloaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## display raw output\n",
    "# preprocessed_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## display image to sanity check\n",
    "display(Image.open(downloaded_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add batch dimension\n",
    "preprocessed_image = np.expand_dims(preprocessed_image, axis=0) \n",
    "\n",
    "# Get predictions\n",
    "predictions = model(preprocessed_image)\n",
    "class_probabilities = tf.nn.softmax(predictions)  # Get probabilities\n",
    "predicted_class = tf.argmax(class_probabilities).numpy()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {class_probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a single predicted class label, using the index of the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted_class_index = tf.argmax(class_probabilities).numpy()\n",
    "# print(f\"Predicted Class Index: {predicted_class_index}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get predicted class \n",
    "\n",
    "And finally download the `label_map.json` to lookup the predicted class name when making prediction so we have a useful output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(\"label_map.json\")\n",
    "label_map_json_string = blob.download_as_string()\n",
    "label_map = json.loads(label_map_json_string)\n",
    "\n",
    "predicted_class_index = tf.argmax(class_probabilities, axis=-1).numpy()[0]  # Extract scalar value\n",
    "# Get the predicted class name (assuming you have predicted_class_index)\n",
    "predicted_class_name = [label for label, index in label_map.items() if index == predicted_class_index][0]\n",
    "\n",
    "print(\"Input File:       \", downloaded_file, \"\\n\")\n",
    "print(\"=\" * 30, \" Prediction Response\", \"=\" * 30) \n",
    "print(\"\\n\", f\"Predicted Class Name: {predicted_class_name}\", \"\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleanup downloaded image\n",
    "\n",
    "Delete the downloaded image file to keep local directory clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(downloaded_file):  # Check if the file exists\n",
    "    os.remove(downloaded_file)\n",
    "    print(f\"Deleted downloaded image file: {downloaded_file}\")\n",
    "else:\n",
    "    print(f\"Downloaded image file not found: {downloaded_file}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
