{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-01: Imagen + Vertex AI Custom Training Job with Tensorflow with Generated Dataset\n",
    "\n",
    "This notebook will generate images from Vertex AI Imagen API and save them into a Google Cloud storage bucket. By default it generates 100 images and can be modified to generate 1,000 images or more.\n",
    "\n",
    "It will then perform transfer learning or fine-tune a pre-trained image classification model using [Google Cloud Vertex AI](https://cloud.google.com/vertex-ai), [TensorFlow](https://www.tensorflow.org/) and [TensorFlow Hub](https://www.tensorflow.org/hub) with data generated from [02_image_classification_model.ipynb](src/02_image_classification_model.ipynb).\n",
    "\n",
    "References:\n",
    "\n",
    "\n",
    "* [Generate images using text prompts  |  Generative AI on Vertex AI  |  Google Cloud](https://cloud.google.com/vertex-ai/generative-ai/docs/image/generate-images)\n",
    "* [Imagen API  |  Generative AI on Vertex AI  |  Google Cloud](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api)\n",
    "* [Transfer learning with TensorFlow Hub  |  TensorFlow Core](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n",
    "\n",
    "by: Justin Marciszewski | justinjm@google.com | AI/ML Specialist CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites \n",
    "\n",
    "1. Setup Google Cloud project (See [README.md](README.md) for full details )\n",
    "2. Create Vertex AI Workbench instance (See [README.md](README.md) for full details )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "\n",
    "Run the cell below to check if required packages are installed.\n",
    "\n",
    "If any are not, they will be installed and kernel will automatically restart and show a notificaiton.\n",
    "\n",
    "If they are already installed, nothing will happen and proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "    ('PIL', 'Pillow'),\n",
    "    ('tensorflow', 'tensorflow'),\n",
    "    ('sklearn', 'scikit-learn'),\n",
    "    ('gcsfs', 'gcsfs')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us-central1\"  \n",
    "REGION = 'us-central1' \n",
    "\n",
    "SERIES = \"03-imagen-vertex-ai\"\n",
    "EXPERIMENT = \"03-imagen-tf\"\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fruit-and-veg-image-model-imagen\"\n",
    "\n",
    "DATASET_VALIDATION_SPLIT = 0.2  # 20% for validation\n",
    "MACHINE_TYPE = 'n1-standard-8'\n",
    "\n",
    "REPO_NAME = \"fruit-veg-image-model-imagen-repo\"\n",
    "IMAGE_NAME = \"tf_training\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "MODEL_URI = \"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/5\" \n",
    "\n",
    "DESIRED_LABELS = [\n",
    "    \"bellpepper_ripe\", \"apple_ripe\", \"banana_ripe\", \n",
    "    \"bellpepper_rotten\", \"apple_rotten\", \"banana_rotten\"]\n",
    "\n",
    "NUM_CLASSES = len(DESIRED_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json \n",
    "\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform, storage\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.vision_models import ImageGenerationModel\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import timeit \n",
    "\n",
    "import random\n",
    "from PIL import Image  \n",
    "import numpy as np\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "URI = f\"gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}\" \n",
    "DIR = f\"temp/{EXPERIMENT}\"\n",
    "\n",
    "LOCAL_DATA_DIR = f\"{DIR}/data\"\n",
    "LOCAL_CSV_IMAGE_DATA_PATH = f\"{LOCAL_DATA_DIR}/labels.csv\"\n",
    "\n",
    "DATASET_CSV = f\"{URI}/{TIMESTAMP}/labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRAMEWORK = 'tf'\n",
    "TASK = 'classification'\n",
    "MODEL_TYPE = 'tl'\n",
    "EXPERIMENT_NAME = f'experiment-{SERIES}-{EXPERIMENT}-{FRAMEWORK}-{TASK}-{MODEL_TYPE}'\n",
    "RUN_NAME = f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set user_flag for testing vs production\n",
    "\n",
    "Test will generate 100 images and incur roughly 2$ spend.\n",
    "\n",
    "Prod will generated 1000 images and incur roughly 20$ spend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## [TODO] - SET USER FLAG (either 'test' or 'prod') ###########################\n",
    "user_flag = 'test'\n",
    "\n",
    "# Basic input validation\n",
    "if user_flag not in ['test', 'prod']:\n",
    "    raise ValueError(\"Invalid input. Please enter either 'test' or 'prod'.\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"User flag set to: {user_flag}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a local directories for staging files \n",
    "\n",
    "* data files from creating labels.csv\n",
    "* build files for creating custom container and running a custom job \n",
    "* model training output files and example input images for local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf $LOCAL_DATA_DIR\n",
    "! mkdir -p $LOCAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIR}/build\"):\n",
    "    os.makedirs(f\"{DIR}/build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIR}/output\"):\n",
    "    os.makedirs(f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "generation_model = ImageGenerationModel.from_pretrained(\"imagegeneration@006\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"aiplatform.googleapis.com\")%%bash\n",
    "# 1. Get the list of CURRENTLY ENABLED services\n",
    "enabled_services=$(gcloud services list --enabled | awk '{print $1}')\n",
    "\n",
    "# 2. Services we WANT to ensure are enabled\n",
    "services_to_enable=(\"storage.googleapis.com\" \"artifactregistry.googleapis.com\" \"cloudbuild.googleapis.com\" \"aiplatform.googleapis.com\")\n",
    "\n",
    "# 3. Check each desired service against the enabled list\n",
    "for service in \"${services_to_enable[@]}\"; do\n",
    "    if ! echo \"$enabled_services\" | grep -q \"$service\"; then\n",
    "        echo \"Enabling $service...\"\n",
    "        gcloud services enable \"$service\"\n",
    "    else\n",
    "        echo \"$service is already enabled.\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Create Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_and_create_bucket(bucket_name, location):\n",
    "    try:\n",
    "        gcs.get_bucket(bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "    except NotFound:\n",
    "        bucket = gcs.create_bucket(bucket_or_name=bucket_name, location=location)\n",
    "        print(f\"Bucket {bucket_name} created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create storage bucket if needed \n",
    "\n",
    "* First run - uncomment the cell  and run it  \n",
    "* Second or great run - comment the cell out to ensure bucket contents are not overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_and_create_bucket(BUCKET_NAME, LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, set the bucket we created using the client above for use in the image generation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = gcs.bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set image generation parameters \n",
    "\n",
    "Set the parameters to build random prompts that in turn generate images. \n",
    "\n",
    "The below can be modified for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "background = [\"white\", \"cookie sheet\", \"cutting board\"]\n",
    "objects = [\"bellpepper_ripe\", \"apple_ripe\", \"banana_ripe\", \"bellpepper_rotten\", \"apple_rotten\", \"banana_rotten\"]\n",
    "images_per_prompt = 1\n",
    "if user_flag == 'test':\n",
    "    num_prompts = 100\n",
    "elif user_flag == 'prod':\n",
    "    num_prompts = 1000\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Ready to generate {num_prompts} images\")\n",
    "print(f\"Estimated cost: ${num_prompts*.02}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_counter = 1  \n",
    "counter_lock = threading.Lock() \n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(GoogleAPIError))\n",
    "def upload_to_gcs(img_byte_arr, blob_name):\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_file(img_byte_arr, content_type=\"image/jpeg\")\n",
    "\n",
    "\n",
    "def generate_and_upload_image(chosen_background, chosen_object, prompt, num_prompts):\n",
    "    global image_counter\n",
    "\n",
    "    response = generation_model.generate_images(prompt=prompt, number_of_images=images_per_prompt)\n",
    "\n",
    "    if response and response.images:\n",
    "        for i, image_data in enumerate(response.images):\n",
    "            pil_image = Image.open(BytesIO(image_data._image_bytes))\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            blob_name = f\"data/{chosen_object}_{chosen_background}_{random.randint(10000, 99999)}_{timestamp}.jpg\".replace(' ', '_')\n",
    "\n",
    "            img_byte_arr = BytesIO()\n",
    "            pil_image.save(img_byte_arr, format=\"JPEG\")\n",
    "            img_byte_arr.seek(0)\n",
    "            try:\n",
    "                upload_to_gcs(img_byte_arr, blob_name)\n",
    "                with counter_lock:\n",
    "                    print(f\"Image {image_counter} of {num_prompts}: '{blob_name}' uploaded to GCS.\")\n",
    "                    image_counter += 1\n",
    "            except Exception as e:  \n",
    "                print(f\"Error processing image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        for _ in range(num_prompts):\n",
    "            chosen_background = random.choice(background)\n",
    "            chosen_object = random.choice(objects)\n",
    "            \n",
    "            chosen_object_food = chosen_object.split(\"_\")[0]\n",
    "            chosen_object_quality = chosen_object.split(\"_\")[1]\n",
    "            prompt = f\"Draw an image of {chosen_object_food} that is {chosen_object_quality} and on a {chosen_background} and photorealistic photography\" \n",
    "\n",
    "            executor.submit(generate_and_upload_image, chosen_background, chosen_object, prompt, num_prompts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Image Generation\n",
    "\n",
    "The next cell executes the image generation and will be a long running operation.\n",
    "\n",
    "Leave this notebook open and do not close it to ensure the job completes.\n",
    "\n",
    "Re-run the cell below if the job stops due to unforeseen error(s).\n",
    "\n",
    "If for some reason the cell below does not re-run successfuly, go to the top navigation bar of the workbench instnace and click: \"Kernel > Restart Kernel & clear all outputs\" then re-run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "print(\"Started image generation job at: \", \n",
    "      datetime.fromtimestamp(datetime.now().timestamp()).strftime(\"%H:%M:%S\"))\n",
    "print(f\"Generating {num_prompts} total images\")\n",
    "\n",
    "main()\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print(\"Completed image generation job at: \", \n",
    "      datetime.fromtimestamp(datetime.now().timestamp()).strftime(\"%H:%M:%S\"))\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Executed the function in {round(execution_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create csv labels file and upload for use in model training\n",
    "\n",
    "Create a csv file called `labels.csv` with the schema:  `gs://filename.jpg, label` \n",
    "\n",
    "This file should contain no headers and be located in GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_list(bucket_name):\n",
    "    # get list of all files from bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs()\n",
    "    file_list = ['gs://' + bucket_name + '/' + blob.name for blob in blobs]\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_list = get_file_list(BUCKET_NAME)\n",
    "file_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataframe(file_list):\n",
    "    # filter to include on filenames with jpeg filename\n",
    "    jpeg_files = [file for file in file_list if file.endswith(('.jpg', '.jpeg'))]\n",
    "    df = pd.DataFrame(jpeg_files, columns=['filename'])\n",
    "    # Extract the label by first removing the GCS prefix, then splitting the blobname, and finally extracting the relevant part\n",
    "    df['label'] = df['filename'].apply(lambda x: '_'.join(x.split('/')[-1].split('_')[:2]))   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data = create_dataframe(file_list)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data.to_csv(LOCAL_CSV_IMAGE_DATA_PATH, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/labels.csv\") \n",
    "blob.upload_from_filename(LOCAL_CSV_IMAGE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Preprocessing ---\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def create_tfrecord_from_gcs(gcs_path, label, tfrecord_writer, storage_client, label_map):\n",
    "    \"\"\"Creates a TFRecord from an image in GCS and writes it to the writer.\"\"\"\n",
    "    try:\n",
    "        bucket_name, blob_name = gcs_path[5:].split('/', 1)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            blob.download_to_filename(temp_file.name)\n",
    "            with open(temp_file.name, 'rb') as image_file:\n",
    "                image_string = image_file.read()\n",
    "\n",
    "        # Convert label to integer using the label map\n",
    "        label_int = label_map[label]\n",
    "\n",
    "        feature = {\n",
    "            'image/encoded': _bytes_feature(image_string),\n",
    "            'image/class/label': _int64_feature(label_int)\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        tfrecord_writer.write(example.SerializeToString())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gcs_path}: {e}\")\n",
    "\n",
    "\n",
    "def create_and_upload_tfrecord(split_name, df, bucket, storage_client, label_map):\n",
    "    \"\"\"Creates a TFRecord file from the DataFrame and uploads it to GCS.\"\"\"\n",
    "    blob_name = f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/{split_name}.tfrecord\"\n",
    "    blob = bucket.blob(blob_name)\n",
    "    writer_lock = threading.Lock()\n",
    "    error_occurred = False  # Flag to track errors\n",
    "\n",
    "    with tempfile.NamedTemporaryFile() as temp_file:\n",
    "        writer = tf.io.TFRecordWriter(temp_file.name)\n",
    "        for row in df.itertuples():\n",
    "            try:\n",
    "                create_tfrecord_from_gcs(row.image_path, row.label, writer, storage_client, label_map)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {row.image_path}: {e}\")\n",
    "                error_occurred = True  # Set the flag if an error occurs\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        # Check if any errors occurred before uploading\n",
    "        if not error_occurred:\n",
    "            blob.upload_from_filename(temp_file.name, timeout=600) # to avoid any connection timeout issues\n",
    "            print(f\"Uploaded {blob_name} to GCS.\")\n",
    "        else:\n",
    "            print(f\"Not uploading {blob_name} due to errors during TFRecord creation.\")\n",
    "\n",
    "# --- Main Preprocessing Function ---\n",
    "def preprocess_data():\n",
    "    \"\"\"Reads CSV, splits data, creates label map, and creates/uploads TFRecords.\"\"\"\n",
    "    df = pd.read_csv(DATASET_CSV, header=None, names=['image_path', 'label'])\n",
    "    train_df, val_df = train_test_split(df, test_size=DATASET_VALIDATION_SPLIT, random_state=42)\n",
    "\n",
    "    # Create a label map (dictionary mapping labels to integer IDs)\n",
    "    unique_labels = df['label'].unique()\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "    # save label map to GCS for use in prediction later \n",
    "    blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/label_map.json\")  # Specify GCS path for label_map.json here\n",
    "    blob.upload_from_string(json.dumps(label_map), content_type='application/json')\n",
    "\n",
    "    create_and_upload_tfrecord('train', train_df, bucket, storage_client, label_map)\n",
    "    create_and_upload_tfrecord('val', val_df, bucket, storage_client, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute pre-processing \n",
    "\n",
    "Create TFRecords files in GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create training files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{DIR}/build/train.py\", 'w') as f:\n",
    "    f.write(\"\"\"import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Get Environment Variables\n",
    "TFRECORD_PATH = os.environ['AIP_TFRECORD_PATH']\n",
    "VALIDATION_PATH = os.environ['AIP_VALIDATION_PATH']\n",
    "OUTPUT_PATH = os.environ['AIP_OUTPUT_PATH']\n",
    "MODEL_URI = os.environ['MODEL_URI']\n",
    "NUM_CLASSES = os.environ['NUM_CLASSES']\n",
    "\n",
    "# sanity check TF version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Load the TFRecords\n",
    "def parse_tfrecord(example):\n",
    "    feature_description = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = tf.io.decode_jpeg(example['image/encoded'], channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])  # Resize to match model input\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    label = tf.cast(example['image/class/label'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return tf.data.TFRecordDataset(filename).map(parse_tfrecord).shuffle(1000).batch(32).prefetch(1)\n",
    "\n",
    "train_dataset = get_dataset(TFRECORD_PATH)\n",
    "val_dataset = get_dataset(VALIDATION_PATH)\n",
    "\n",
    "# Load the TensorFlow Hub model\n",
    "model = hub.KerasLayer(MODEL_URI, trainable=False)\n",
    "\n",
    "# Add your custom classification head\n",
    "### [CHECK] Ensure this is same number of classes in your dataset ### <-- [CHECK] ###\n",
    "print(f\"NUM_CLASSES: {NUM_CLASSES}\")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "# Save the model\n",
    "model.save(OUTPUT_PATH)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dockerfile\n",
    "\n",
    "Refs:\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/training/pre-built-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{DIR}/build/Dockerfile\", 'w') as f:\n",
    "    f.write(\"\"\"FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /root\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install tensorflow-hub==0.12.0\n",
    "\n",
    "# Copy the training script\n",
    "COPY train.py /root/train.py\n",
    "\n",
    "# Define the entry point\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Container Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create docker repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create {REPO_NAME} --repository-format=docker --location={REGION} --description=\"Docker repository for fruit and vegetable image model training on Vertex AI with Imagen images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optional: uncomment if you'd like to sanity check \n",
    "# ! gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### configure auth for docker\n",
    "\n",
    "Before you push or pull container images, configure Docker to use the `gcloud` command-line tool to authenticate requests to `Artifact Registry` for your region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit build of container image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --region={REGION} --tag={IMAGE_URI} --timeout=1h ./{DIR}/build "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Job Definition \n",
    "\n",
    "\n",
    "Refs \n",
    "\n",
    "* [Create custom training jobs  |  Vertex AI  |  Google Cloud](https://cloud.google.com/vertex-ai/docs/training/create-custom-job#create_custom_job-python_vertex_ai_sdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_custom_job(project_id, \n",
    "                      region, \n",
    "                      display_name, \n",
    "                      bucket_name,\n",
    "                      base_output_dir,\n",
    "                      machine_type,\n",
    "                      labels\n",
    "                     ):\n",
    "    \n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": IMAGE_URI,\n",
    "                \"env\": [  \n",
    "                    {\"name\": \"AIP_TFRECORD_PATH\", \"value\": f\"{URI}/{TIMESTAMP}/train.tfrecord\"},\n",
    "                    {\"name\": \"AIP_VALIDATION_PATH\", \"value\": f\"{URI}/{TIMESTAMP}/val.tfrecord\"},\n",
    "                    {\"name\": \"AIP_OUTPUT_PATH\", \"value\": f\"{base_output_dir}/output\"},\n",
    "                    {\"name\": \"AIP_MODEL_DIR\", \"value\": f\"{base_output_dir}/output\"},  # <- [CHECK] Important! the model output location\n",
    "                    {\"name\": \"TFHUB_CACHE_DIR\", \"value\": f\"{base_output_dir}/tfhub_cache\"}, \n",
    "                    {\"name\": \"MODEL_URI\", \"value\": MODEL_URI},\n",
    "                    {\"name\": \"NUM_CLASSES\", \"value\": str(NUM_CLASSES)},\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    custom_job = aiplatform.CustomJob(\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        display_name=display_name,\n",
    "        staging_bucket=bucket_name,\n",
    "        base_output_dir=base_output_dir,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    return custom_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_job = create_custom_job(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    display_name = f'{SERIES}_{EXPERIMENT}_{TIMESTAMP}',\n",
    "    bucket_name = BUCKET_NAME,  # aka - staging_bucket \n",
    "    base_output_dir = f\"{URI}/models/{TIMESTAMP}\", \n",
    "    machine_type=MACHINE_TYPE,\n",
    "    labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}', 'run_name' : f'{RUN_NAME}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Submit the custom job to Vertex AI\n",
    "custom_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model for local inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions \n",
    "\n",
    "For downloading model, a sample image and finally making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_blobs_with_prefix(bucket_name, prefix, local_directory):\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Skip \"directory\" objects\n",
    "        if blob.name.endswith(\"/\"):\n",
    "            continue\n",
    "\n",
    "        # Calculate the relative path within the prefix\n",
    "        relative_path = blob.name[len(prefix):] \n",
    "\n",
    "        # Create the local directory for the relative path\n",
    "        local_file_directory = os.path.join(local_directory, os.path.dirname(relative_path))\n",
    "        os.makedirs(local_file_directory, exist_ok=True)\n",
    "\n",
    "        # Download the blob\n",
    "        local_file_path = os.path.join(local_directory, relative_path)\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        print(f\"Blob {blob.name} downloaded to {local_file_path}.\")\n",
    "\n",
    "def download_random_jpg(bucket_name, pattern):\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    # Get list of blobs (files) with the pattern\n",
    "    blobs = [blob for blob in bucket.list_blobs() if re.search(pattern, blob.name)]\n",
    "    \n",
    "    if not blobs:\n",
    "        print(\"No files found with the pattern:\", pattern)\n",
    "        return None\n",
    "    \n",
    "    # Choose a random blob\n",
    "    random_blob = random.choice(blobs)\n",
    "\n",
    "    # Download the blob\n",
    "    local_filename = random_blob.name \n",
    "    local_directory = os.path.dirname(local_filename)\n",
    "    os.makedirs(local_directory, exist_ok=True)  # Ensure directory exists\n",
    "    \n",
    "    random_blob.download_to_filename(local_filename)\n",
    "    print(f\"Downloaded {local_filename} from bucket {bucket_name}\")\n",
    "\n",
    "    return local_filename\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Preprocesses an image for model prediction.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize & set to float32\n",
    "    return img_array  # Remove extra dimension (model handles batching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_blobs_with_prefix(bucket_name=BUCKET_NAME, \n",
    "                           prefix=f\"{SERIES}/{EXPERIMENT}/models/{TIMESTAMP}/output/\", \n",
    "                           local_directory=f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Prepare an image for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## download a random image \n",
    "downloaded_file = download_random_jpg(\n",
    "    bucket_name=BUCKET_NAME, \n",
    "    pattern=f'({\"|\".join(DESIRED_LABELS)})(?!\\.png$)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## and pre-process image for prediction\n",
    "preprocessed_image = preprocess_image(downloaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## display raw output\n",
    "# preprocessed_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## display image to sanity check\n",
    "display(Image.open(downloaded_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add batch dimension\n",
    "preprocessed_image = np.expand_dims(preprocessed_image, axis=0) \n",
    "\n",
    "# Get predictions\n",
    "predictions = model(preprocessed_image)\n",
    "class_probabilities = tf.nn.softmax(predictions)  # Get probabilities\n",
    "predicted_class = tf.argmax(class_probabilities).numpy()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {class_probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a single predicted class label, using the index of the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted_class_index = tf.argmax(class_probabilities).numpy()\n",
    "print(f\"Predicted Class Index: {predicted_class_index}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get predicted class \n",
    "\n",
    "And finally download the `label_map.json` to lookup the predicted class name when making prediction so we have a useful output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/label_map.json\")\n",
    "label_map_json_string = blob.download_as_string()\n",
    "label_map = json.loads(label_map_json_string)\n",
    "\n",
    "predicted_class_index = tf.argmax(class_probabilities, axis=-1).numpy()[0]  # Extract scalar value\n",
    "# Get the predicted class name (assuming you have predicted_class_index)\n",
    "predicted_class_name = [label for label, index in label_map.items() if index == predicted_class_index][0]\n",
    "\n",
    "print(\"Input File:       \", downloaded_file, \"\\n\")\n",
    "print(\"=\" * 30, \" Prediction Response\", \"=\" * 30) \n",
    "print(\"\\n\", f\"Predicted Class Name: {predicted_class_name}\", \"\\n\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleanup downloaded image\n",
    "\n",
    "Delete the downloaded image file to keep local directory clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(downloaded_file):  # Check if the file exists\n",
    "    os.remove(downloaded_file)\n",
    "    print(f\"Deleted downloaded image file: {downloaded_file}\")\n",
    "else:\n",
    "    print(f\"Downloaded image file not found: {downloaded_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
