{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fruit and Vegetable Disease (Healthy vs Rotten) - Kaggle + Vertex AI Training (Custom Job) Example\n",
    "\n",
    "* Kaggle page:  https://www.kaggle.com/datasets/muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten\n",
    "* dataset: https://www.kaggle.com/datasets/muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten/data\n",
    "* notebook: https://www.kaggle.com/code/osamaabobakr/fruit-and-vegetable-disease-healthy-vs-rotten\n",
    "\n",
    "by: Justin Marciszewski | justinjm@google.com | AI/ML Specialist CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "packages = [\n",
    "    ('numpy', 'numpy'),\n",
    "    ('os', 'os-sys'), # os is built-in, this is for demonstration\n",
    "    ('cv2', 'opencv-python'),\n",
    "    ('re', 're'), # re is built-in, this is for demonstration\n",
    "    ('random', 'random'), # random is built-in, this is for demonstration\n",
    "    ('matplotlib.pyplot', 'matplotlib'),\n",
    "    ('seaborn', 'seaborn'),\n",
    "    ('kaggle.api.kaggle_api_extended', 'kaggle'),\n",
    "    ('sklearn.model_selection', 'scikit-learn'),\n",
    "    ('sklearn.utils', 'scikit-learn'),\n",
    "    ('keras', 'keras'),\n",
    "    ('tensorflow.keras', 'tensorflow'),\n",
    "    ('tensorflow.keras.layers', 'tensorflow'),\n",
    "    ('tensorflow.keras.models', 'tensorflow'),\n",
    "    ('tensorflow.keras.applications', 'tensorflow'),\n",
    "    ('tensorflow.keras.preprocessing.image', 'tensorflow')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    try:\n",
    "        importlib.import_module(package[0])\n",
    "    except ImportError:\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "\n",
    "if install:\n",
    "    print(\"Installation of missing packages complete. Please run the next cell to restart the kernel before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "### Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us-central1\"  \n",
    "REGION = 'us-central1' \n",
    "\n",
    "EXPERIMENT = \"03\"\n",
    "SERIES = \"fruit-and-vegetable-image-model\"\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID \n",
    "\n",
    "# data pre-processing\n",
    "DATASET_VALIDATION_SPLIT = 0.2  # 20% for validation\n",
    "\n",
    "## custom containar \n",
    "REPO_NAME = \"fruit-and-vegetable-image-model-repo\"\n",
    "IMAGE_NAME = \"tf_training\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "## model training \n",
    "DESIRED_LABELS = [\n",
    "    'Apple__Healthy', 'Apple__Rotten',\n",
    "    'Banana__Healthy', 'Banana__Rotten',\n",
    "    'Bellpepper__Healthy', 'Bellpepper__Rotten'\n",
    "]\n",
    "NUM_CLASSES = len(DESIRED_LABELS)\n",
    "\n",
    "## Vertex AI custom job\n",
    "MACHINE_TYPE = 'n1-standard-8'\n",
    "MODEL_URI = \"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/5\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Ingestion\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import tempfile\n",
    "import threading\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Data pre-processing\n",
    "from PIL import Image  # For image loading and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modeling \n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "URI = f\"gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}\" # custom job -> base_output_dir = f\"{URI}/models/{TIMESTAMP}\",\n",
    "DIR = f\"temp/{EXPERIMENT}\"\n",
    "\n",
    "LOCAL_DATA_DIR = f\"{DIR}/data\"\n",
    "LOCAL_CSV_IMAGE_DATA_PATH = f\"{LOCAL_DATA_DIR}/labels.csv\"\n",
    "\n",
    "DATASET_CSV = f\"{URI}/{TIMESTAMP}/labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRAMEWORK = 'tf'\n",
    "TASK = 'classification'\n",
    "MODEL_TYPE = 'tl'\n",
    "EXPERIMENT_NAME = f'experiment-{SERIES}-{EXPERIMENT}-{FRAMEWORK}-{TASK}-{MODEL_TYPE}'\n",
    "RUN_NAME = f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a local directories for staging files \n",
    "\n",
    "* data files from creating labels.csv\n",
    "* build files for creating custom container and running a custom job \n",
    "* model training output files and example input images for local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf $LOCAL_DATA_DIR\n",
    "! mkdir -p $LOCAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIR}/build\"):\n",
    "    os.makedirs(f\"{DIR}/build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIR}/output\"):\n",
    "    os.makedirs(f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Google Cloud Storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_and_create_bucket(bucket_name, location):\n",
    "    try:\n",
    "        storage_client.get_bucket(bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "    except NotFound:\n",
    "        bucket = storage_client.create_bucket(bucket_or_name=bucket_name, location=location)\n",
    "        print(f\"Bucket {bucket_name} created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_and_create_bucket(BUCKET_NAME, LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Kaggle credentials\n",
    "\n",
    "You will need a Kaggle account and locate or create a kaggle.json file in the directory: `/home/jupyter/.config/kaggle`\n",
    "\n",
    "Steps:\n",
    "\n",
    "* manually download your credentail file from kaggle.com -> Profile\n",
    "* run this command in terminal to move it to the correct location: `mv kaggle.json .config/kaggle/kaggle.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up Kaggle credentials \n",
    "os.environ['KAGGLE_USERNAME'] = 'YOUR_KAGGLE_USERNAME' \n",
    "os.environ['KAGGLE_KEY'] = 'YOUR_KAGGLE_API_KEY'\n",
    "\n",
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Specify the dataset you want to download\n",
    "dataset_slug = 'muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten'\n",
    "\n",
    "# Download the dataset\n",
    "api.dataset_download_files(dataset_slug, path=LOCAL_DATA_DIR, unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_image_to_rgb_and_jpeg(image_path):\n",
    "    \"\"\"Converts and saves an image to RGB JPEG format, overwriting the original.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        img.save(image_path, format='JPEG')  # Overwrite the original\n",
    "        # print(f'Converted and saved: {image_path}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {image_path}: {e}')\n",
    "\n",
    "def process_directory(root_dir, subdirs_to_convert, max_workers=None):\n",
    "    \"\"\"Processes images within specified subdirectories using multithreading.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            # Filter directories based on the provided list\n",
    "            dirs[:] = [d for d in dirs if d in subdirs_to_convert]\n",
    "\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):  # Add more extensions if needed\n",
    "                    image_path = Path(root) / file\n",
    "                    executor.submit(convert_image_to_rgb_and_jpeg, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = f\"{LOCAL_DATA_DIR}/Fruit And Vegetable Diseases Dataset\"\n",
    "subdirectories_to_convert = DESIRED_LABELS\n",
    "\n",
    "process_directory(root_directory, subdirectories_to_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop over each subdirectory (label) and copy the contents using gsutil\n",
    "for subdir in DESIRED_LABELS:\n",
    "    source = f'\"{LOCAL_DATA_DIR}/Fruit And Vegetable Diseases Dataset/{subdir}/*\"'\n",
    "    destination = f\"{URI}/data/{subdir}/\"\n",
    "    print(destination)\n",
    "    command = f\"gsutil -m cp -r {source} {destination} > /dev/null 2>&1\"\n",
    "    \n",
    "    # Execute the command using subprocess\n",
    "    process = subprocess.run(command, shell=True)\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(f\"Successfully copied {subdir}\")\n",
    "    else:\n",
    "        print(f\"Failed to copy {subdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv labels file and upload for use in model training\n",
    "\n",
    "Create a csv file called `labels.csv` with the schema:  `gs://filename.jpg, label` \n",
    "\n",
    "This file should contain no headers and be located in GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_list(bucket_name):\n",
    "    # get list of all files from bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs()\n",
    "    file_list = ['gs://' + bucket_name + '/' + blob.name for blob in blobs]\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_list = get_file_list(BUCKET_NAME)\n",
    "file_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataframe(file_list, filter_pattern):\n",
    "    # filter to include on filenames with jpg filename\n",
    "    image_files = [file for file in file_list if file.endswith(('.jpg'))]\n",
    "    df = pd.DataFrame(image_files, columns=['filename'])\n",
    "    \n",
    "    ## filter to only 3 foods per constants set above for demonstration purposes \n",
    "    df = df[df['filename'].str.contains(filter_pattern, regex=True)]\n",
    "    \n",
    "    # Extract the label from the GCS path (it's the second part after the bucket name)\n",
    "    df['label'] = df['filename'].apply(lambda x: x.split('/')[6])  # Assuming the label is in the ith segment of the path\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100 # set option to view long strings \n",
    "\n",
    "df_labels = create_dataframe(file_list, \n",
    "                             filter_pattern = '|'.join(DESIRED_LABELS))\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save labels.csv\n",
    "\n",
    "Save labels.csv locally and to GCS Bucket for use in vertex ai training in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## DEV - use only 100 images for dev\n",
    "# df_labels.head(100).to_csv(LOCAL_CSV_IMAGE_DATA_PATH, index=False, header=False)\n",
    "df_labels.to_csv(LOCAL_CSV_IMAGE_DATA_PATH, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/labels.csv\")  # works? \n",
    "blob.upload_from_filename(LOCAL_CSV_IMAGE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Preprocessing ---\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def create_tfrecord_from_gcs(gcs_path, label, tfrecord_writer, storage_client, label_map):\n",
    "    \"\"\"Creates a TFRecord from an image in GCS and writes it to the writer.\"\"\"\n",
    "    try:\n",
    "        bucket_name, blob_name = gcs_path[5:].split('/', 1)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            blob.download_to_filename(temp_file.name)\n",
    "            with open(temp_file.name, 'rb') as image_file:\n",
    "                image_string = image_file.read()\n",
    "\n",
    "        # Convert label to integer using the label map\n",
    "        label_int = label_map[label]\n",
    "\n",
    "        feature = {\n",
    "            'image/encoded': _bytes_feature(image_string),\n",
    "            'image/class/label': _int64_feature(label_int)\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        tfrecord_writer.write(example.SerializeToString())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gcs_path}: {e}\")\n",
    "\n",
    "\n",
    "def create_and_upload_tfrecord(split_name, df, bucket, storage_client, label_map):\n",
    "    \"\"\"Creates a TFRecord file from the DataFrame and uploads it to GCS.\"\"\"\n",
    "    blob_name = f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/{split_name}.tfrecord\"\n",
    "    blob = bucket.blob(blob_name)\n",
    "    writer_lock = threading.Lock()\n",
    "    error_occurred = False  # Flag to track errors\n",
    "\n",
    "    with tempfile.NamedTemporaryFile() as temp_file:\n",
    "        writer = tf.io.TFRecordWriter(temp_file.name)\n",
    "        for row in df.itertuples():\n",
    "            try:\n",
    "                create_tfrecord_from_gcs(row.image_path, row.label, writer, storage_client, label_map)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {row.image_path}: {e}\")\n",
    "                error_occurred = True  # Set the flag if an error occurs\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        # Check if any errors occurred before uploading\n",
    "        if not error_occurred:\n",
    "            blob.upload_from_filename(temp_file.name, timeout=600) # to avoid any connection timeout issues\n",
    "            print(f\"Uploaded {blob_name} to GCS.\")\n",
    "        else:\n",
    "            print(f\"Not uploading {blob_name} due to errors during TFRecord creation.\")\n",
    "\n",
    "# --- Main Preprocessing Function ---\n",
    "def preprocess_data():\n",
    "    \"\"\"Reads CSV, splits data, creates label map, and creates/uploads TFRecords.\"\"\"\n",
    "    df = pd.read_csv(DATASET_CSV, header=None, names=['image_path', 'label'])\n",
    "    train_df, val_df = train_test_split(df, test_size=DATASET_VALIDATION_SPLIT, random_state=42)\n",
    "\n",
    "    # Create a label map (dictionary mapping labels to integer IDs)\n",
    "    unique_labels = df['label'].unique()\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "    # save label map to GCS for use in prediction later \n",
    "    blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/label_map.json\")  # Specify GCS path for label_map.json here\n",
    "    blob.upload_from_string(json.dumps(label_map), content_type='application/json')\n",
    "\n",
    "    create_and_upload_tfrecord('train', train_df, bucket, storage_client, label_map)\n",
    "    create_and_upload_tfrecord('val', val_df, bucket, storage_client, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute data pre-processing \n",
    "\n",
    "Create TFRecords files in GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{DIR}/build/train.py\", 'w') as f:\n",
    "    f.write(\"\"\"import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Get Environment Variables\n",
    "TFRECORD_PATH = os.environ['AIP_TFRECORD_PATH']\n",
    "VALIDATION_PATH = os.environ['AIP_VALIDATION_PATH']\n",
    "OUTPUT_PATH = os.environ['AIP_OUTPUT_PATH']\n",
    "MODEL_URI = os.environ['MODEL_URI']\n",
    "NUM_CLASSES = os.environ['NUM_CLASSES']\n",
    "\n",
    "# sanity check TF version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Load the TFRecords\n",
    "def parse_tfrecord(example):\n",
    "    feature_description = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = tf.io.decode_jpeg(example['image/encoded'], channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])  # Resize to match model input\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    label = tf.cast(example['image/class/label'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return tf.data.TFRecordDataset(filename).map(parse_tfrecord).shuffle(1000).batch(32).prefetch(1)\n",
    "\n",
    "train_dataset = get_dataset(TFRECORD_PATH)\n",
    "val_dataset = get_dataset(VALIDATION_PATH)\n",
    "\n",
    "# Load the TensorFlow Hub model\n",
    "model = hub.KerasLayer(MODEL_URI, trainable=False)\n",
    "\n",
    "# Add your custom classification head\n",
    "# CHECK - ensure this is same number of classes in your dataset ###########################\n",
    "print(f\"NUM_CLASSES: {NUM_CLASSES}\")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "# Save the model\n",
    "model.save(OUTPUT_PATH)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile\n",
    "\n",
    "Refs:\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/training/pre-built-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{DIR}/build/Dockerfile\", 'w') as f:\n",
    "    f.write(\"\"\"FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /root\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install tensorflow-hub==0.12.0\n",
    "\n",
    "# Copy the training script\n",
    "COPY train.py /root/train.py\n",
    "\n",
    "# Define the entry point\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Container Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create {REPO_NAME} --repository-format=docker --location={REGION} --description=\"Docker repository for fruit and vegetable image model training on Vertex AI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure auth for docker\n",
    "\n",
    "Before you push or pull container images, configure Docker to use the `gcloud` command-line tool to authenticate requests to `Artifact Registry` for your region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --region={REGION} --tag={IMAGE_URI} --timeout=1h ./{DIR}/build "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Job Definition \n",
    "\n",
    "Refs \n",
    "\n",
    "* [Create custom training jobs  |  Vertex AI  |  Google Cloud](https://cloud.google.com/vertex-ai/docs/training/create-custom-job#create_custom_job-python_vertex_ai_sdk)\n",
    "* customJob - https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs#CustomJob\n",
    "    * customJobSpec - `baseOutputDirectory` https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec\n",
    "* other example: https://github.com/statmike/vertex-ai-mlops/blob/main/05%20-%20TensorFlow/05c%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Custom%20Job%20With%20Custom%20Container.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_custom_job(project_id, \n",
    "                      region, \n",
    "                      display_name, \n",
    "                      bucket_name,\n",
    "                      base_output_dir,\n",
    "                      machine_type,\n",
    "                      labels\n",
    "                     ):\n",
    "    \n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": IMAGE_URI,\n",
    "                \"env\": [  \n",
    "                    {\"name\": \"AIP_TFRECORD_PATH\", \"value\": f\"{URI}/{TIMESTAMP}/train.tfrecord\"},\n",
    "                    {\"name\": \"AIP_VALIDATION_PATH\", \"value\": f\"{URI}/{TIMESTAMP}/val.tfrecord\"},\n",
    "                    {\"name\": \"AIP_OUTPUT_PATH\", \"value\": f\"{base_output_dir}/output\"},\n",
    "                    {\"name\": \"AIP_MODEL_DIR\", \"value\": f\"{base_output_dir}/output\"},  # <- important! the model output location\n",
    "                    {\"name\": \"TFHUB_CACHE_DIR\", \"value\": f\"{base_output_dir}/tfhub_cache\"}, \n",
    "                    {\"name\": \"MODEL_URI\", \"value\": MODEL_URI},\n",
    "                    {\"name\": \"NUM_CLASSES\", \"value\": str(NUM_CLASSES)},\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    custom_job = aiplatform.CustomJob(\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        display_name=display_name,\n",
    "        staging_bucket=bucket_name,\n",
    "        base_output_dir=base_output_dir,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    return custom_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_job = create_custom_job(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    display_name = f'{SERIES}_{EXPERIMENT}_{TIMESTAMP}',\n",
    "    bucket_name = BUCKET_NAME,  # aka - staging_bucket \n",
    "    base_output_dir = f\"{URI}/models/{TIMESTAMP}\", \n",
    "    machine_type=MACHINE_TYPE,\n",
    "    labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}', 'run_name' : f'{RUN_NAME}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Submit the custom job to Vertex AI\n",
    "custom_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model for local inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper functions \n",
    "\n",
    "For downloading model, a sample image and finally making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_blobs_with_prefix(bucket_name, prefix, local_directory):\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Skip \"directory\" objects\n",
    "        if blob.name.endswith(\"/\"):\n",
    "            continue\n",
    "\n",
    "        # Calculate the relative path within the prefix\n",
    "        relative_path = blob.name[len(prefix):] \n",
    "\n",
    "        # Create the local directory for the relative path\n",
    "        local_file_directory = os.path.join(local_directory, os.path.dirname(relative_path))\n",
    "        os.makedirs(local_file_directory, exist_ok=True)\n",
    "\n",
    "        # Download the blob\n",
    "        local_file_path = os.path.join(local_directory, relative_path)\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        print(f\"Blob {blob.name} downloaded to {local_file_path}.\")\n",
    "\n",
    "        \n",
    "def download_random_jpg(bucket_name, pattern):\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    # Get list of blobs (files) with the pattern\n",
    "    blobs = [blob for blob in bucket.list_blobs() if re.search(pattern, blob.name)]\n",
    "    \n",
    "    if not blobs:\n",
    "        print(\"No files found with the pattern:\", pattern)\n",
    "        return None\n",
    "    \n",
    "    # Choose a random blob\n",
    "    random_blob = random.choice(blobs)\n",
    "\n",
    "    # Download the blob\n",
    "    local_filename = random_blob.name \n",
    "    local_directory = os.path.dirname(local_filename)\n",
    "    os.makedirs(local_directory, exist_ok=True)  # Ensure directory exists\n",
    "    \n",
    "    random_blob.download_to_filename(local_filename)\n",
    "    print(f\"Downloaded {local_filename} from bucket {bucket_name}\")\n",
    "\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Preprocesses an image for model prediction.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize & set to float32\n",
    "    return img_array  # Remove extra dimension (model handles batching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_blobs_with_prefix(bucket_name=BUCKET_NAME, \n",
    "                           prefix=f\"{SERIES}/{EXPERIMENT}/models/{TIMESTAMP}/output/\", \n",
    "                           local_directory=f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare an image for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download a random image \n",
    "\n",
    "Filter to only 3 foods for demonstration purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same set of labels as before\n",
    "downloaded_file = download_random_jpg(\n",
    "    bucket_name=BUCKET_NAME, \n",
    "    pattern=f'({\"|\".join(DESIRED_LABELS)})(?!\\.png$)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## display image to sanity check\n",
    "display(Image.open(downloaded_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## and pre-process image for prediction\n",
    "preprocessed_image = preprocess_image(downloaded_file)\n",
    "# preprocessed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add batch dimension\n",
    "preprocessed_image = np.expand_dims(preprocessed_image, axis=0) \n",
    "\n",
    "# Get predictions\n",
    "predictions = model(preprocessed_image)\n",
    "class_probabilities = tf.nn.softmax(predictions)  # Get probabilities\n",
    "predicted_class = tf.argmax(class_probabilities).numpy()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {class_probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get predicted class \n",
    "\n",
    "And finally download the `label_map.json` to lookup the predicted class name when making prediction so we have a useful output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/label_map.json\")\n",
    "label_map_json_string = blob.download_as_string()\n",
    "label_map = json.loads(label_map_json_string)\n",
    "\n",
    "predicted_class_index = tf.argmax(class_probabilities, axis=-1).numpy()[0]  # Extract scalar value\n",
    "# Get the predicted class name (assuming you have predicted_class_index)\n",
    "predicted_class_name = [label for label, index in label_map.items() if index == predicted_class_index][0]\n",
    "print(f\"Predicted Class Name: {predicted_class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup downloaded image\n",
    "\n",
    "Delete the downloaded image file to keep local directory clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(downloaded_file):  # Check if the file exists\n",
    "    os.remove(downloaded_file)\n",
    "    print(f\"Deleted downloaded image file: {downloaded_file}\")\n",
    "else:\n",
    "    print(f\"Downloaded image file not found: {downloaded_file}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5045111,
     "sourceId": 8463025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
