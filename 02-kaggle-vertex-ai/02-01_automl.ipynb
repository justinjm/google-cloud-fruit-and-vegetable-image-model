{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 02-01: Kaggle + Vertex AI AutoML with Fruit and Vegetable Disease (Healthy vs Rotten) Dataset\n",
    "\n",
    "Train an image classification model using [Google Cloud Vertex AI](https://cloud.google.com/vertex-ai) and AutoML with data from [Kaggle](https://www.kaggle.com/datasets/muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten/data).\n",
    "\n",
    "\n",
    "* Kaggle page:  https://www.kaggle.com/datasets/muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten\n",
    "* dataset: https://www.kaggle.com/datasets/muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten/data\n",
    "* notebook: https://www.kaggle.com/code/osamaabobakr/fruit-and-vegetable-disease-healthy-vs-rotten\n",
    "\n",
    "by: Justin Marciszewski | justinjm@google.com | AI/ML Specialist CE\n",
    "\n",
    "refs:\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/training-overview\n",
    "* https://cloud.google.com/vertex-ai/docs/tutorials/image-classification-automl/overview\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/automl_image_classification_online_prediction.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "packages = [\n",
    "    ('numpy', 'numpy'),\n",
    "    ('cv2', 'opencv-python'),\n",
    "    ('matplotlib.pyplot', 'matplotlib'),\n",
    "    ('seaborn', 'seaborn'),\n",
    "    ('kaggle.api.kaggle_api_extended', 'kaggle'),\n",
    "    ('sklearn.model_selection', 'scikit-learn'),\n",
    "    ('sklearn.utils', 'scikit-learn'),\n",
    "    ('keras', 'keras'),\n",
    "    ('tensorflow.keras', 'tensorflow'),\n",
    "    ('tensorflow.keras.layers', 'tensorflow'),\n",
    "    ('tensorflow.keras.models', 'tensorflow'),\n",
    "    ('tensorflow.keras.applications', 'tensorflow'),\n",
    "    ('tensorflow.keras.preprocessing.image', 'tensorflow')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    try:\n",
    "        importlib.import_module(package[0])\n",
    "    except ImportError:\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "\n",
    "if install:\n",
    "    print(\"Installation of missing packages complete. Please run the next cell to restart the kernel before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "### Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us-central1\"  \n",
    "REGION = 'us-central1' \n",
    "\n",
    "SERIES = \"02-kaggle-vertex-ai\"\n",
    "EXPERIMENT = \"02-automl\" # notebook number \n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fruit-and-veg-image-model\"\n",
    "\n",
    "## model training \n",
    "DESIRED_LABELS = [\n",
    "    'Apple__Healthy', 'Apple__Rotten',\n",
    "    'Banana__Healthy', 'Banana__Rotten',\n",
    "    'Bellpepper__Healthy', 'Bellpepper__Rotten'\n",
    "]\n",
    "NUM_CLASSES = len(DESIRED_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Ingestion\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import tempfile\n",
    "import threading\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Data pre-processing\n",
    "from PIL import Image  # For image loading and preprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Modeling \n",
    "from google.cloud import aiplatform\n",
    "import base64\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "print(f\"TIMESTAMP: {TIMESTAMP}\")\n",
    "URI = f\"gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}\" \n",
    "DIR = f\"temp/{EXPERIMENT}\"\n",
    "\n",
    "LOCAL_DATA_DIR = f\"{DIR}/data\"\n",
    "LOCAL_CSV_IMAGE_DATA_PATH = f\"{LOCAL_DATA_DIR}/labels.csv\"\n",
    "\n",
    "DATASET_CSV = f\"{URI}/{TIMESTAMP}/labels.csv\"\n",
    "\n",
    "DATASET_DISPLAY_NAME = f\"{SERIES}-{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRAMEWORK = 'tf'\n",
    "TASK = 'classification'\n",
    "MODEL_TYPE = 'tl'\n",
    "EXPERIMENT_NAME = f'experiment-{SERIES}-{EXPERIMENT}-{FRAMEWORK}-{TASK}-{MODEL_TYPE}'\n",
    "RUN_NAME = f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a local directories for staging files \n",
    "\n",
    "* data files from creating labels.csv\n",
    "* build files for creating custom container and running a custom job \n",
    "* model training output files and example input images for local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf $LOCAL_DATA_DIR\n",
    "! mkdir -p $LOCAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIR}/build\"):\n",
    "    os.makedirs(f\"{DIR}/build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIR}/output\"):\n",
    "    os.makedirs(f\"{DIR}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_and_create_bucket(bucket_name, location):\n",
    "    try:\n",
    "        storage_client.get_bucket(bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "    except NotFound:\n",
    "        bucket = storage_client.create_bucket(bucket_or_name=bucket_name, location=location)\n",
    "        print(f\"Bucket {bucket_name} created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_and_create_bucket(BUCKET_NAME, LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Kaggle credentials\n",
    "\n",
    "You will need a Kaggle account and locate or create a kaggle.json file in the directory: `/home/jupyter/.config/kaggle`\n",
    "\n",
    "Steps:\n",
    "\n",
    "* manually download your credentail file from kaggle.com -> Profile\n",
    "* run this command in terminal to move it to the correct location: `mv kaggle.json .config/kaggle/kaggle.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up Kaggle credentials \n",
    "os.environ['KAGGLE_USERNAME'] = 'YOUR_KAGGLE_USERNAME' \n",
    "os.environ['KAGGLE_KEY'] = 'YOUR_KAGGLE_API_KEY'\n",
    "\n",
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Specify the dataset you want to download\n",
    "dataset_slug = 'muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten'\n",
    "\n",
    "# Download the dataset\n",
    "api.dataset_download_files(dataset_slug, path=LOCAL_DATA_DIR, unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_image_to_rgb_and_jpeg(image_path):\n",
    "    \"\"\"Converts and saves an image to RGB JPEG format, overwriting the original.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        img.save(image_path, format='JPEG')  # Overwrite the original\n",
    "        # print(f'Converted and saved: {image_path}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {image_path}: {e}')\n",
    "\n",
    "def process_directory(root_dir, subdirs_to_convert, max_workers=None):\n",
    "    \"\"\"Processes images within specified subdirectories using multithreading.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            # Filter directories based on the provided list\n",
    "            dirs[:] = [d for d in dirs if d in subdirs_to_convert]\n",
    "\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):  # Add more extensions if needed\n",
    "                    image_path = Path(root) / file\n",
    "                    executor.submit(convert_image_to_rgb_and_jpeg, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = f\"{LOCAL_DATA_DIR}/Fruit And Vegetable Diseases Dataset\"\n",
    "subdirectories_to_convert = DESIRED_LABELS\n",
    "\n",
    "process_directory(root_directory, subdirectories_to_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load to GCS\n",
    "\n",
    "Load only a subset of images (set by the `DESIRED_LABELS` list) for demonstration purposes, update the `DESIRED_LABELS` to include all the images in the Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop over each subdirectory (label) and copy the contents using gsutil\n",
    "for subdir in DESIRED_LABELS:\n",
    "    source = f'\"{LOCAL_DATA_DIR}/Fruit And Vegetable Diseases Dataset/{subdir}/*\"'\n",
    "    destination = f\"{URI}/data/{subdir}/\"\n",
    "    print(destination)\n",
    "    command = f\"gsutil -m cp -r {source} {destination} > /dev/null 2>&1\"\n",
    "    \n",
    "    # Execute the command using subprocess\n",
    "    process = subprocess.run(command, shell=True)\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(f\"Successfully copied {subdir}\")\n",
    "    else:\n",
    "        print(f\"Failed to copy {subdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data \n",
    "\n",
    "refs:\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create csv labels file and upload for use in model training\n",
    "\n",
    "Create a csv file called `labels.csv` with the schema:  `gs://filename.jpg, label` \n",
    "\n",
    "This file should contain no headers and be located in GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_list(bucket_name):\n",
    "    # get list of all files from bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs()\n",
    "    file_list = ['gs://' + bucket_name + '/' + blob.name for blob in blobs]\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_list = get_file_list(BUCKET_NAME)\n",
    "file_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataframe(file_list, filter_pattern):\n",
    "    # filter to include on filenames with jpg filename\n",
    "    image_files = [file for file in file_list if file.endswith(('.jpg'))]\n",
    "    df = pd.DataFrame(image_files, columns=['filename'])\n",
    "    \n",
    "    ## filter to only 3 foods per constants set above for demonstration purposes \n",
    "    df = df[df['filename'].str.contains(filter_pattern, regex=True)]\n",
    "    \n",
    "    # Extract the label from the GCS path (it's the second part after the bucket name)\n",
    "    df['label'] = df['filename'].apply(lambda x: x.split('/')[6])  # Assuming the label is in the ith segment of the path\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100 # set option to view long strings \n",
    "\n",
    "df_labels = create_dataframe(file_list, \n",
    "                             filter_pattern = '|'.join(DESIRED_LABELS))\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save labels.csv\n",
    "\n",
    "Save labels.csv locally and to GCS Bucket for use in vertex ai training in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labels.to_csv(LOCAL_CSV_IMAGE_DATA_PATH, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{TIMESTAMP}/labels.csv\")\n",
    "blob.upload_from_filename(LOCAL_CSV_IMAGE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vertex AI Dataset\n",
    "\n",
    "Create a managed Vertex AI dataset. \n",
    "\n",
    "refs:\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/image-data/classification/create-dataset#aiplatform_create_dataset_image_sample-python_vertex_ai_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiplatform.ImageDataset.create(\n",
    "        display_name=f\"{SERIES}_{EXPERIMENT}_{TIMESTAMP}\",\n",
    "        gcs_source=[DATASET_CSV],\n",
    "        import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification, \n",
    "        sync=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Submit the AutoML training job to Vertex AI\n",
    "\n",
    "refs\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/image-data/classification/train-model#aiplatform_create_training_pipeline_image_classification_sample-python_vertex_ai_sdk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.AutoMLImageTrainingJob(\n",
    "    display_name=f\"{SERIES}_{EXPERIMENT}_{TIMESTAMP}\",\n",
    "    model_type=\"CLOUD\",\n",
    "    prediction_type=\"classification\",\n",
    "    multi_label=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## manual set here if needed \n",
    "# dataset = aiplatform.ImageDataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=f\"{SERIES}_{EXPERIMENT}_{TIMESTAMP}\",\n",
    "    training_fraction_split=0.4,\n",
    "    validation_fraction_split=0.3,\n",
    "    test_fraction_split=0.3,\n",
    "    budget_milli_node_hours=8000,\n",
    "    disable_early_stopping=False,\n",
    "    sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Evaluate your AutoML image classification model here if needed so that you can iterate on your model.\n",
    "\n",
    "Vertex AI provides model evaluation metrics to help you determine the performance of your models, such as precision and recall metrics. Vertex AI calculates evaluation metrics by using the [test set](https://cloud.google.com/vertex-ai/docs/general/ml-use).\n",
    "\n",
    "refs:\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/image-data/classification/evaluate-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model\n",
    "    print(\"model object set!\")\n",
    "except NameError:\n",
    "    print(f\"model object not set, fetching...\")\n",
    "    models = aiplatform.Model.list(filter=f\"display_name={SERIES}_{EXPERIMENT}_{TIMESTAMP}\")\n",
    "    model = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_evaluations = model.list_model_evaluations()\n",
    "\n",
    "for model_evaluation in model_evaluations:\n",
    "    print(json.dumps(model_evaluation.to_dict(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions - Online\n",
    "\n",
    "We will now get an an online (real-time) prediction from our model. \n",
    "\n",
    "Online predictions are synchronous requests made to a model endpoint. Use online predictions when you are making requests in response to application input or in situations that require timely inference.\n",
    "\n",
    "You can read more about this at the references linked below.\n",
    "\n",
    "refs:\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/image-data/classification/get-predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a test item\n",
    "\n",
    "Get the first image from our `labels.csv` file to use as a test item to ensure our model returns the expected response.\n",
    "\n",
    "In practice, we would use an image that our trained model has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_item = !gsutil cat $DATASET_CSV | head -n1\n",
    "if len(str(test_item[0]).split(\",\")) == 3:\n",
    "    _, test_item, test_label = str(test_item[0]).split(\",\")\n",
    "else:\n",
    "    test_item, test_label = str(test_item[0]).split(\",\")\n",
    "\n",
    "print(f\"\\nTest Item:\\n\")\n",
    "print(f\"  Test Item Source: {test_item}\\n\")  \n",
    "print(f\"  Test Item Actual Label: {test_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print raw prediction output to see raw model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.io.gfile.GFile(test_item, \"rb\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# The format of each instance should conform to the deployed model's prediction input schema.\n",
    "instances = [{\"content\": base64.b64encode(content).decode(\"utf-8\")}]\n",
    "\n",
    "prediction = endpoint.predict(instances=instances)\n",
    "\n",
    "print(json.dumps(prediction, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print only label with highest confidence score in a pretty way for demonstration purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the relevant data from the prediction response\n",
    "confidences = prediction.predictions[0]['confidences']\n",
    "display_names = prediction.predictions[0]['displayNames']\n",
    "\n",
    "# Find the index of the highest confidence score\n",
    "max_confidence_index = confidences.index(max(confidences))\n",
    "\n",
    "# Extract the label with the highest confidence and its score\n",
    "top_label = display_names[max_confidence_index]\n",
    "top_confidence = confidences[max_confidence_index]\n",
    "\n",
    "# Print the result in a pretty format\n",
    "print(f\"\\nPrediction Result:\\n\")\n",
    "print(f\"  Top Label: {top_label}\")\n",
    "print(f\"  Confidence: {top_confidence:2f}\\n\")  \n",
    "\n",
    "print(f\"\\nTest Item Actuals:\\n\")\n",
    "print(f\"  Test Item Actual Label: {test_label}\")\n",
    "print(f\"  Test Item Source: {test_item}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleanup !!danger zone!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# undeploy endpoint only \n",
    "# endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ! warning - running the code below deletes objects that require \n",
    "## long running processes to recreate\n",
    "\n",
    "# Delete the dataset using the Vertex dataset object\n",
    "## dataset.delete()\n",
    "\n",
    "# Delete the endpoint using the Vertex endpoint object\n",
    "## endpoint.delete()\n",
    "\n",
    "# Delete the model using the Vertex model object\n",
    "##model.delete()\n",
    "\n",
    "# Delete the AutoML trainig job\n",
    "##job.delete()\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "## delete_bucket = False  # Set True for deletion\n",
    "## if delete_bucket:\n",
    "##     ! gsutil -m rm -r gs://$BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5045111,
     "sourceId": 8463025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
